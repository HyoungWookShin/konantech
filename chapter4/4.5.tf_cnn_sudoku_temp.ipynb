{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_fix = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, mode):\n",
    "        self.mode = mode\n",
    "        self.input_dim = self.output_dim = self.train_count = 0\n",
    "        self.train_xs = self.test_xs = self.validate_xs = []\n",
    "        self.train_ys = self.test_ys = self.validate_ys = []\n",
    "        self.target_names = []\n",
    "    \n",
    "    def get_train_data(self, batch_size, nth):\n",
    "        if nth == 0:\n",
    "            self.indices = np.arange(self.train_count)\n",
    "            np.random.shuffle(self.indices)\n",
    "        \n",
    "        from_idx = nth * batch_size\n",
    "        to_idx = (nth + 1) * batch_size\n",
    "    \n",
    "        train_X = self.train_xs[self.indices[from_idx:to_idx]]\n",
    "        train_Y = self.train_ys[self.indices[from_idx:to_idx]]\n",
    "        \n",
    "        return train_X, train_Y    \n",
    "    \n",
    "    def get_test_data(self, validate=False, count=0):\n",
    "        if validate:\n",
    "            xs, ys = self.validate_xs, self.validate_ys\n",
    "        else:\n",
    "            xs, ys = self.test_xs, self.test_ys\n",
    "        \n",
    "        if count == 0:\n",
    "            return xs, ys\n",
    "        \n",
    "        if count > len(xs): count = len(xs)\n",
    "        \n",
    "        indices = np.arange(len(xs))\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        return xs[indices[0:count]], ys[indices[0:count]]\n",
    "    \n",
    "    def get_target_name(self, idxs):\n",
    "        return self.target_names[idxs]\n",
    "    \n",
    "    def demonstrate(self, x, estimate, answer):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "class IrisDataset(Dataset):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iris_init(self, mode, train_ratio=0.8):\n",
    "    Dataset.__init__(self, mode)\n",
    "    \n",
    "    dataset = datasets.load_iris()\n",
    "    \n",
    "    xs, ys = iris_prepare(self, mode, dataset.data, dataset.target)\n",
    "    \n",
    "    data_count = len(dataset.data)\n",
    "    self.train_count = int(data_count * train_ratio)\n",
    "\n",
    "    if random_fix: np.random.seed(1234)\n",
    "\n",
    "    indices = np.arange(data_count)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    self.train_xs = xs[indices[0:self.train_count]]\n",
    "    self.train_ys = ys[indices[0:self.train_count]]\n",
    "    self.test_xs = self.validate_xs = xs[indices[self.train_count:]]\n",
    "    self.test_ys = self.validate_ys = ys[indices[self.train_count:]]\n",
    "    \n",
    "    self.target_names = dataset.target_names\n",
    "\n",
    "def iris_prepare(self, mode, data, target):\n",
    "    if mode == \"regression\":\n",
    "        self.input_dim = 3\n",
    "        self.output_dim = 1\n",
    "        xs = data[:, 0:3]\n",
    "        ys = data[:, 3:4]\n",
    "    elif mode == \"binary\":\n",
    "        self.input_dim = 4\n",
    "        self.output_dim = 1\n",
    "        xs = data\n",
    "        ys = np.equal(target, 0).astype(\"float32\").reshape(-1,1)\n",
    "    elif mode == \"select\":\n",
    "        self.input_dim = 4\n",
    "        self.output_dim = 3\n",
    "        xs = data\n",
    "        ys = np.eye(3)[target]\n",
    "        \n",
    "    return xs, ys\n",
    "\n",
    "IrisDataset.__init__ = iris_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iris_demonstrate(self, x, estimate, answer):\n",
    "    if self.mode == \"regression\":\n",
    "        print(\"({}, {}, {}) => 추정 {:3.1f}, 정답: {:3.1f}\".format(x[0], x[1], x[2], estimate, answer))\n",
    "    elif self.mode == \"binary\":\n",
    "        estr = \"is setosa\"\n",
    "        astr = \"(오답)\"\n",
    "        if not estimate: estr = \"is not setosa\"\n",
    "        if estimate == answer: astr = \"(정답)\"\n",
    "        print(\"({}, {}, {}, {}) => {} {}\".format(x[0], x[1], x[2], x[3], estr, astr))\n",
    "    elif self.mode == \"select\":\n",
    "        estr = self.target_names[estimate]\n",
    "        astr = \"({})\".format(self.target_names[answer])\n",
    "        if estimate == answer: astr = \"(정답)\"\n",
    "        print(\"({}, {}, {}, {}) => {} {}\".format(x[0], x[1], x[2], x[3], estr, astr))\n",
    "    \n",
    "IrisDataset.demonstrate = iris_demonstrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "id1 = IrisDataset(\"regression\")\n",
    "id2 = IrisDataset(\"binary\", 0.88)\n",
    "id3 = IrisDataset(\"select\", 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MnistDataset(Dataset):\n",
    "    pass\n",
    "\n",
    "def mnist_init(self):\n",
    "    Dataset.__init__(self, \"select\")\n",
    "    \n",
    "    dataset = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "        \n",
    "    self.train_xs = dataset.train.images\n",
    "    self.train_ys = dataset.train.labels\n",
    "        \n",
    "    self.validate_xs = dataset.validation.images\n",
    "    self.validate_ys = dataset.validation.labels\n",
    "        \n",
    "    self.test_xs = dataset.test.images\n",
    "    self.test_ys = dataset.test.labels\n",
    "\n",
    "    self.input_dim = 28 * 28\n",
    "    self.output_dim = 10\n",
    "    \n",
    "    self.train_count = len(dataset.train.images)\n",
    "    self.target_names = ['0','1','2','3','4','5','6','7','8','9']\n",
    "\n",
    "def mnist_demonstrate(self, xs, est, ans):\n",
    "    rows, cols = 4, 4\n",
    "    f, a = plt.subplots(rows, cols, figsize=(cols, rows))\n",
    "    for row in range(cols):\n",
    "        for col in range(cols):\n",
    "            i = row * cols + col\n",
    "            estr = self.target_names[est[i]]\n",
    "            astr = self.target_names[ans[i]]\n",
    "            if est[i] == ans[i]:\n",
    "                caption = \"{}\".format(estr)\n",
    "            else:\n",
    "                caption = \"{}=>{}\".format(astr, estr)\n",
    "            a[row][col].axvspan(0, 0, 0, 6.0)\n",
    "            a[row][col].imshow(np.reshape(xs[i], (28,28)))\n",
    "            a[row][col].text(0.5, -1.5, caption)\n",
    "            a[row][col].axis('off')\n",
    "    f.show()\n",
    "    plt.draw()\n",
    "    plt.show()\n",
    "        \n",
    "MnistDataset.__init__ = mnist_init\n",
    "MnistDataset.demonstrate = mnist_demonstrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "md = MnistDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, name, dataset):\n",
    "        self.name = name\n",
    "        self.dataset = dataset\n",
    "        \n",
    "    def train(self, epoch_count=10, batch_size=10):\n",
    "        pass\n",
    "    \n",
    "    def test(self):\n",
    "        pass\n",
    "    \n",
    "    def demonstrate(self, num=10, batch=False):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class MultiLayerPerceptronModel(Model):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def mlp_model_train(self, epoch_count=10, batch_size=10):\n",
    "    if batch_size == 0:\n",
    "        batch_size = self.dataset.train_count\n",
    "        \n",
    "    batch_count = int(self.dataset.train_count / batch_size)\n",
    "    report_period = epoch_count / 10\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    if random_fix: np.random.seed(1945)\n",
    "    \n",
    "    time1 = time2 = int(time.time())\n",
    "    \n",
    "    print(\"Model {} train report:\".format(self.name))\n",
    "    \n",
    "    dset = self.dataset\n",
    "    run_targets = [self.train_op, self.loss, self.accuracy]\n",
    "\n",
    "    for epoch in range(epoch_count):\n",
    "        costs = []\n",
    "        accs = []\n",
    "        for n in range(batch_count):\n",
    "            train_X, train_Y = dset.get_train_data(batch_size, n)\n",
    "            _, cost, acc = sess.run(run_targets, \\\n",
    "                feed_dict={self.x:train_X, self.y:train_Y})\n",
    "            costs.append(cost)\n",
    "            accs.append(acc)\n",
    "            \n",
    "        if (epoch+1) % report_period == 0:\n",
    "            validate_X, validate_Y = dset.get_test_data(True, 30)\n",
    "            acc = sess.run(self.accuracy, \\\n",
    "                feed_dict={self.x:validate_X, self.y:validate_Y})\n",
    "            time3 = int(time.time())\n",
    "            print(\"    Epoch {}: cost={:5.3f}, \\\n",
    "accuracy={:5.3f}/{:5.3f} ({}/{} secs)\". \\\n",
    "                  format(epoch+1, np.mean(costs), np.mean(accs), \\\n",
    "                         acc, time3-time2, time3-time1))\n",
    "            time2 = time3\n",
    "\n",
    "    print(\"\")\n",
    "    \n",
    "    path = \"params/{}.ckpt\".format(self.name)\n",
    "    self.saver.save(sess, path)\n",
    "    sess.close()\n",
    "\n",
    "MultiLayerPerceptronModel.train = mlp_model_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model_test(self):\n",
    "    test_X, test_Y = self.dataset.get_test_data()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    path = \"params/{}.ckpt\".format(self.name)\n",
    "    self.saver.restore(sess, path)\n",
    "    \n",
    "    time1 = int(time.time())\n",
    "    acc = sess.run(self.accuracy, feed_dict={self.x:test_X, self.y:test_Y})\n",
    "    time2 = int(time.time())\n",
    "    \n",
    "    print(\"Model {} test report: accuracy = {:5.3f}, ({} secs)\".format(self.name, acc, time2-time1))\n",
    "    print(\"\")\n",
    "    \n",
    "    sess.close()\n",
    "\n",
    "MultiLayerPerceptronModel.test = mlp_model_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model_demonstrate(self, num=10, batch=False):\n",
    "    demo_X, demo_Y = self.dataset.get_test_data(False, num)\n",
    "\n",
    "    sess = tf.Session()\n",
    "    path = \"params/{}.ckpt\".format(self.name)\n",
    "    self.saver.restore(sess, path)\n",
    "    \n",
    "    print(\"Model {} Demonstration\".format(self.name))\n",
    "    est, ans, probs = sess.run([self.estimate, self.answer, self.probs], \\\n",
    "           feed_dict={self.x:demo_X, self.y:demo_Y})\n",
    "    if batch:\n",
    "        self.dataset.demonstrate(demo_X, demo_Y, est, ans, probs)\n",
    "    else:\n",
    "        for n in range(len(demo_X)):\n",
    "            self.dataset.demonstrate(demo_X[n], demo_Y[n], est[n], ans[n], probs[n])\n",
    "        print(\"\")\n",
    "    \n",
    "    sess.close()\n",
    "    \n",
    "MultiLayerPerceptronModel.demonstrate = mlp_model_demonstrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_init(self, name, dataset, hidden_dims, learning_rate=0.001):\n",
    "    Model.__init__(self, name, dataset)\n",
    "    with tf.variable_scope(self.name):\n",
    "        self.build_placeholders()\n",
    "        self.build_parameters(hidden_dims)\n",
    "        self.build_neuralnet(hidden_dims)\n",
    "        self.build_loss_accuracy()\n",
    "        build_optimizer(self, learning_rate)\n",
    "        build_saver(self)\n",
    "\n",
    "MultiLayerPerceptronModel.__init__ = mlp_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_placeholders(self):\n",
    "    input_dim = self.dataset.input_dim\n",
    "    output_dim = self.dataset.output_dim\n",
    "    self.x = tf.placeholder(\"float\", [None, input_dim])\n",
    "    self.y = tf.placeholder(\"float\", [None, output_dim])\n",
    "\n",
    "MultiLayerPerceptronModel.build_placeholders = build_placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_parameters(self, hidden_dims):\n",
    "    if random_fix: np.random.seed(9876)\n",
    "\n",
    "    input_dim = self.dataset.input_dim\n",
    "    output_dim = self.dataset.output_dim\n",
    "\n",
    "    self.w_hids, self.b_hids = [], []\n",
    "    \n",
    "    prev_dim = input_dim\n",
    "\n",
    "    for n in range(len(hidden_dims)):\n",
    "        next_dim = hidden_dims[n]\n",
    "        w = tf.Variable(init_rand_normal(prev_dim, next_dim))\n",
    "        b = tf.Variable(tf.zeros([next_dim]))\n",
    "        self.w_hids.append(w)\n",
    "        self.b_hids.append(b)\n",
    "        prev_dim = next_dim\n",
    "\n",
    "    self.w_out = tf.Variable(init_rand_normal(prev_dim, output_dim))\n",
    "    self.b_out = tf.Variable(tf.zeros([output_dim]))\n",
    "    \n",
    "def init_rand_normal(in_dim, out_dim, rand_std=0.0300):\n",
    "    if not random_fix:\n",
    "        init = tf.random_normal([in_dim, out_dim], stddev=rand_std)\n",
    "    else:\n",
    "        init_64 = np.random.normal(0, rand_std, [in_dim, out_dim])\n",
    "        init = init_64.astype('float32')\n",
    "\n",
    "    return init\n",
    "        \n",
    "MultiLayerPerceptronModel.build_parameters = build_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_neuralnet(self, hidden_dims):\n",
    "    hidden = self.x\n",
    "    for n in range(len(hidden_dims)):\n",
    "        affine = tf.matmul(hidden, self.w_hids[n]) + self.b_hids[n]\n",
    "        hidden = tf.nn.relu(affine)\n",
    "    self.output = tf.matmul(hidden, self.w_out) + self.b_out\n",
    "        \n",
    "MultiLayerPerceptronModel.build_neuralnet = build_neuralnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loss_accuracy(self):\n",
    "    if self.dataset.mode == \"regression\":\n",
    "        self.estimate = self.output[:,0]\n",
    "        self.answer = self.y[:,0]\n",
    "        diff = self.estimate - self.answer\n",
    "        self.probs = tf.constant(0)\n",
    "        self.loss = tf.reduce_mean(tf.pow(diff, 2))\n",
    "        error = tf.reduce_mean(tf.abs(diff) / self.answer)\n",
    "        self.accuracy = 1 - error\n",
    "    elif self.dataset.mode == \"binary\":\n",
    "        cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.y, logits=self.output)\n",
    "        self.loss = tf.reduce_mean(cross_entropy)\n",
    "        self.probs = tf.nn.sigmoid(output)\n",
    "        self.estimate = tf.greater(self.output, 0)\n",
    "        self.answer = tf.equal(self.y, 1.0)\n",
    "        correct_bool = tf.equal(self.estimate, self.answer)\n",
    "        correct = tf.cast(correct_bool, \"float\")\n",
    "        self.accuracy = tf.reduce_mean(correct)\n",
    "    elif self.dataset.mode == \"select\":\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=self.y, logits=self.output)\n",
    "        self.loss = tf.reduce_mean(cross_entropy)\n",
    "        self.probs = tf.nn.softmax(output)\n",
    "        self.estimate = tf.argmax(self.output, 1)\n",
    "        self.answer = tf.argmax(self.y, 1)\n",
    "        correct_bool = tf.equal(self.estimate, self.answer)\n",
    "        correct = tf.cast(correct_bool, \"float\")\n",
    "        self.accuracy = tf.reduce_mean(correct)\n",
    "        \n",
    "MultiLayerPerceptronModel.build_loss_accuracy = build_loss_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(self, learning_rate):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    self.train_op = optimizer.minimize(self.loss)\n",
    "\n",
    "def build_saver(self):\n",
    "    var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
    "    self.saver = tf.train.Saver(var_list=var_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def relu_derv(y):\n",
    "    return np.sign(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derv(x, y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "def sigmoid_cross_entropy(z, x):\n",
    "    return np.maximum(x, 0) - x * z + np.log(1 + np.exp(-np.abs(x)))\n",
    "\n",
    "def sigmoid_cross_entropy_derv(z, x):\n",
    "    return -z + sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    max_elem = np.max(x, axis=1)\n",
    "    diff = (x.transpose() - max_elem).transpose()\n",
    "    exp = np.exp(diff)\n",
    "    sum_exp = np.sum(exp, axis=1)\n",
    "    probs = (exp.transpose() / sum_exp).transpose()\n",
    "    return probs\n",
    "\n",
    "def softmax_derv(x, y):\n",
    "    mb_size, nom_size = x.shape\n",
    "    derv = np.ndarray([mb_size, nom_size, nom_size])\n",
    "    for n in range(mb_size):\n",
    "        for i in range(nom_size):\n",
    "            for j in range(nom_size):\n",
    "                derv[n, i, j] = -y[n,i] * y[n,j]\n",
    "            derv[n, i, i] += y[n,i]\n",
    "    return derv\n",
    "\n",
    "def softmax_cross_entropy(p, q):\n",
    "    return -np.sum(p * np.log(q), axis=1)\n",
    "\n",
    "def softmax_cross_entropy_derv(p, q):\n",
    "    return -p / q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoTFMLPModel(Model):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notf_init(self,name,dataset,hidden_dims,learning_rate=0.001):\n",
    "    Model.__init__(self, name, dataset)\n",
    "    init_parameters(self, hidden_dims)\n",
    "    self.learning_rate = learning_rate\n",
    "    self.hidden_dims = hidden_dims\n",
    "    \n",
    "NoTFMLPModel.__init__ = notf_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def notf_model_train(self, epoch_count=10, batch_size=10):\n",
    "    if batch_size == 0:\n",
    "        batch_size = self.dataset.train_count\n",
    "        \n",
    "    batch_count = int(self.dataset.train_count / batch_size)\n",
    "    report_period = epoch_count / 10\n",
    "    \n",
    "    if random_fix: np.random.seed(1945)\n",
    "    \n",
    "    time1 = time2 = int(time.time())\n",
    "    \n",
    "    print(\"Model {} train report:\".format(self.name))\n",
    "    \n",
    "    dset = self.dataset\n",
    "\n",
    "    for epoch in range(epoch_count):\n",
    "        costs = []\n",
    "        accs = []\n",
    "        for n in range(batch_count):\n",
    "            train_X, train_Y = dset.get_train_data(batch_size, n)\n",
    "            cost, acc = self.train_step(train_X, train_Y)\n",
    "            costs.append(cost)\n",
    "            accs.append(acc)\n",
    "            \n",
    "        if (epoch+1) % report_period == 0:\n",
    "            validate_X, validate_Y = dset.get_test_data(True, 30)\n",
    "            acc = self.get_accuracy(validate_X, validate_Y)\n",
    "            time3 = int(time.time())\n",
    "            print(\"    Epoch {}: cost={:5.3f}, \\\n",
    "accuracy={:5.3f}/{:5.3f} ({}/{} secs)\". \\\n",
    "                  format(epoch+1, np.mean(costs), np.mean(accs), \\\n",
    "                         acc, time3-time2, time3-time1))\n",
    "            time2 = time3\n",
    "\n",
    "    print(\"\")\n",
    "    \n",
    "    path = \"params/{}.ntpt\".format(self.name)\n",
    "    self.save_parameters(path)\n",
    "\n",
    "NoTFMLPModel.train = notf_model_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notf_model_test(self):\n",
    "    test_X, test_Y = self.dataset.get_test_data()\n",
    "\n",
    "    path = \"params/{}.ntpt\".format(self.name)\n",
    "    self.restore_parameters(path)\n",
    "    \n",
    "    time1 = int(time.time())\n",
    "    acc = self.get_accuracy(test_X, test_Y)\n",
    "    time2 = int(time.time())\n",
    "    \n",
    "    print(\"Model {} test report: accuracy = {:5.3f}, ({} secs)\". \\\n",
    "          format(self.name, acc, time2-time1))\n",
    "    print(\"\")\n",
    "    \n",
    "NoTFMLPModel.test = notf_model_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notf_model_demonstrate(self, num=10, batch=False):\n",
    "    demo_X, demo_Y = self.dataset.get_test_data(False, num)\n",
    "\n",
    "    path = \"params/{}.ntpt\".format(self.name)\n",
    "    self.restore_parameters(path)\n",
    "    \n",
    "    print(\"Model {} Demonstration\".format(self.name))\n",
    "    est, ans = self.get_estimate_answer(demo_X, demo_Y)\n",
    "    if batch:\n",
    "        self.dataset.demonstrate(demo_X, est, ans)\n",
    "    else:\n",
    "        for n in range(len(demo_X)):\n",
    "            self.dataset.demonstrate(demo_X[n], est[n], ans[n])\n",
    "        print(\"\")\n",
    "    \n",
    "NoTFMLPModel.demonstrate = notf_model_demonstrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(self, x, y):\n",
    "    output = self.proc_forward(x)\n",
    "\n",
    "    loss_grad = 1.0\n",
    "    \n",
    "    if self.dataset.mode == \"regression\":\n",
    "        diff = output - y\n",
    "        power = np.power(diff, 2)\n",
    "        loss = np.mean(power)\n",
    "        power_derv = np.ones_like(y) / np.prod(y.shape)\n",
    "        power_grad = power_derv * loss_grad\n",
    "        diff_derv = 2 * diff\n",
    "        diff_grad = diff_derv * power_grad\n",
    "        output_derv = 1\n",
    "        output_grad = output_derv * diff_grad\n",
    "    elif self.dataset.mode == \"binary\":\n",
    "        entropy = sigmoid_cross_entropy(y, output)\n",
    "        loss = np.mean(entropy)\n",
    "        ent_derv = np.ones_like(entropy) / np.prod(entropy.shape)\n",
    "        ent_grad = ent_derv * loss_grad\n",
    "        output_derv = sigmoid_cross_entropy_derv(y, output)\n",
    "        output_grad = output_derv * ent_grad\n",
    "    elif self.dataset.mode == \"select\":\n",
    "        probs = softmax(output)\n",
    "        entropy = softmax_cross_entropy(y, probs)\n",
    "        loss = np.mean(entropy)\n",
    "        ent_grad = loss_grad / np.prod(entropy.shape)\n",
    "        probs_derv = softmax_cross_entropy_derv(y, probs)\n",
    "        probs_grad = probs_derv * ent_grad\n",
    "        output_derv = softmax_derv(output, probs)\n",
    "        output_grad = [np.matmul(output_derv[n], probs_grad[n]) \\\n",
    "                       for n in range(output.shape[0])]\n",
    "    \n",
    "    self.proc_backward(x, output_grad)\n",
    "    \n",
    "    return loss, self.eval_accuracy(output, y)\n",
    "\n",
    "NoTFMLPModel.train_step = train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters(self, hidden_dims):\n",
    "    if random_fix: np.random.seed(9876)\n",
    "\n",
    "    input_dim = self.dataset.input_dim\n",
    "    output_dim = self.dataset.output_dim\n",
    "\n",
    "    self.weights, self.biases = [], []\n",
    "    \n",
    "    prev_dim = input_dim\n",
    "\n",
    "    for n in range(len(hidden_dims)):\n",
    "        next_dim = hidden_dims[n]\n",
    "        w = init_rand_normal(prev_dim, next_dim)\n",
    "        b = np.zeros([next_dim])\n",
    "        self.weights.append(w)\n",
    "        self.biases.append(b)\n",
    "        prev_dim = next_dim\n",
    "\n",
    "    w = notf_init_rand_normal(prev_dim, output_dim)\n",
    "    b = np.zeros([output_dim])\n",
    "    self.weights.append(w)\n",
    "    self.biases.append(b)\n",
    "    \n",
    "def notf_init_rand_normal(in_dim, out_dim, rand_std=0.0300):\n",
    "    init_64 = np.random.normal(0, rand_std, [in_dim, out_dim])\n",
    "    init = init_64.astype('float32')\n",
    "    return init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_forward(self, x):\n",
    "    self.hiddens = [x]\n",
    "\n",
    "    for n in range(len(self.hidden_dims)):\n",
    "        w, b = self.weights[n], self.biases[n]\n",
    "        hid = relu(np.matmul(self.hiddens[-1], w) + b)\n",
    "        self.hiddens.append(hid)\n",
    "    \n",
    "    w, b = self.weights[-1], self.biases[-1]\n",
    "    output = np.matmul(self.hiddens[-1], w) + b\n",
    "    \n",
    "    return output\n",
    "\n",
    "NoTFMLPModel.proc_forward = proc_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_backward(self, x, grad):\n",
    "    w_out_derv = self.hiddens[-1].transpose()\n",
    "    w_out_grad = np.matmul(w_out_derv, grad)\n",
    "    \n",
    "    b_out_grad = np.sum(grad, axis=0)\n",
    "    \n",
    "    hidden_derv = self.weights[-1].transpose()\n",
    "    hidden_grad = np.matmul(grad, hidden_derv)\n",
    "    \n",
    "    for n in range(len(self.hidden_dims))[::-1]:\n",
    "        hidden_affine_derv = relu_derv(self.hiddens[n+1])\n",
    "        hidden_affine_grad = hidden_affine_derv * hidden_grad\n",
    "    \n",
    "        w_hid_derv = self.hiddens[n].transpose()\n",
    "        w_hid_grad = np.matmul(w_hid_derv, hidden_affine_grad)\n",
    "    \n",
    "        b_hid_grad = np.sum(hidden_affine_grad, axis=0)\n",
    "        \n",
    "        hidden_derv = self.weights[n].transpose()\n",
    "        hidden_grad = np.matmul(hidden_affine_grad, hidden_derv)\n",
    "    \n",
    "        self.weights[n] = self.weights[n] - self.learning_rate * w_hid_grad\n",
    "        self.biases[n] = self.biases[n] - self.learning_rate * b_hid_grad\n",
    "    \n",
    "    self.weights[-1] = self.weights[-1] - self.learning_rate * w_out_grad\n",
    "    self.biases[-1] = self.biases[-1] - self.learning_rate * b_out_grad\n",
    "    \n",
    "NoTFMLPModel.proc_backward = proc_backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(self, x, y):\n",
    "    output = self.proc_forward(x)\n",
    "    return self.eval_accuracy(output, y)\n",
    "\n",
    "def eval_accuracy(self, output, y):\n",
    "    if self.dataset.mode == \"regression\":\n",
    "        diff = output - y\n",
    "        answer = y[:,0]\n",
    "        error = np.mean(np.abs(diff) / answer)\n",
    "        accuracy = 1 - error\n",
    "        probs = 0\n",
    "    elif self.dataset.mode == \"binary\":\n",
    "        #probs = sigmoid(output)\n",
    "        #estimate = np.greater(probs, 0.5)\n",
    "        estimate = np.greater(output, 0)\n",
    "        answer = np.equal(y, 1.0)\n",
    "        correct_bool = np.equal(estimate, answer)\n",
    "        #correct = np.cast(correct_bool, \"float32\")\n",
    "        accuracy = np.mean(correct_bool)\n",
    "    elif self.dataset.mode == \"select\":\n",
    "        #probs = softmax(output)\n",
    "        #estimate = np.argmax(probs, 1)\n",
    "        estimate = np.argmax(output, 1)\n",
    "        answer = np.argmax(y, 1)\n",
    "        correct_bool = np.equal(estimate, answer)\n",
    "        #correct = np.cast(correct_bool, \"float32\")\n",
    "        accuracy = np.mean(correct_bool)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "NoTFMLPModel.eval_accuracy = eval_accuracy\n",
    "NoTFMLPModel.get_accuracy = get_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimate_answer(self, x, y):\n",
    "    output = self.proc_forward(x)\n",
    "\n",
    "    if self.dataset.mode == \"regression\":\n",
    "        estimate = output[:,0]\n",
    "        answer = y[:,0]\n",
    "    elif self.dataset.mode == \"binary\":\n",
    "        #probs = sigmoid(output)\n",
    "        #estimate = np.greater(probs, 0.5)\n",
    "        estimate = np.greater(output, 0)\n",
    "        answer = np.equal(y, 1.0)\n",
    "    elif self.dataset.mode == \"select\":\n",
    "        #probs = softmax(output)\n",
    "        #estimate = np.argmax(probs, 1)\n",
    "        estimate = np.argmax(output, 1)\n",
    "        answer = np.argmax(y, 1)\n",
    "    \n",
    "    return estimate, answer\n",
    "\n",
    "NoTFMLPModel.get_estimate_answer = get_estimate_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_parameters(self, path):\n",
    "    np.savez(path, self.weights, self.biases)\n",
    "\n",
    "def restore_parameters(self, path):\n",
    "    fc = np.load(path)\n",
    "    self.weights, self.biases = fc['arr_0'], fc['arr_1']\n",
    "\n",
    "NoTFMLPModel.save_parameters = save_parameters\n",
    "NoTFMLPModel.restore_parameters = restore_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SudokuDataset(Dataset):\n",
    "    pass\n",
    "\n",
    "def unpack_map(line):\n",
    "    quiz = np.zeros([81], dtype=np.int8)\n",
    "    solution = np.zeros([81], dtype=np.int8)\n",
    "    for n in range(81):\n",
    "        quiz[n] = int(line[n])\n",
    "        solution[n] = int(line[n+82])\n",
    "    return quiz, solution\n",
    "\n",
    "def fill_hints(quiz, solution, hint_cnt):\n",
    "    for n in range(hint_cnt):\n",
    "        for k in range(1000):\n",
    "            pos = np.random.randint(81)\n",
    "            if quiz[pos] == 0:\n",
    "                quiz[pos] = solution[pos]\n",
    "                break\n",
    "    return quiz\n",
    "    \n",
    "def sudoku_init(self, train_ratio=0.80, valid_ratio=0.05):\n",
    "    Dataset.__init__(self, \"sudoku\")\n",
    "    \n",
    "    quizzes, solutions = [], []\n",
    "\n",
    "    max_count = 50000\n",
    "    \n",
    "    for line in open(\"./data/sudoku.csv\"):\n",
    "        if line[0] == 'q': continue\n",
    "        quiz, solution = unpack_map(line)\n",
    "        \n",
    "        #quiz = solution\n",
    "        #for n in range(5):\n",
    "        #    pos = np.random.randint(81)\n",
    "        #    quiz[pos] = 0\n",
    "        #quizzes.append(solution)\n",
    "        #solutions.append(solution)\n",
    "        \n",
    "        open_cnt = int((81-np.count_nonzero(quiz)) / 10)\n",
    "        for n in range(5):\n",
    "            quizzes.append(quiz)\n",
    "            solutions.append(solution)\n",
    "            quiz = fill_hints(quiz, solution, open_cnt)\n",
    "        if len(quizzes) >= max_count: break\n",
    "\n",
    "    ones = np.ones([81]).astype(int)\n",
    "    solution_idxes = solutions - ones\n",
    "    xs = np.asarray(quizzes)\n",
    "    ys = solution_idxes.reshape(-1, 81)\n",
    "    #ys = np.eye(9)[solution_idxes].reshape(-1, 81*9)\n",
    "\n",
    "    self.input_dim = 81\n",
    "    self.output_dim = 81\n",
    "    #self.output_dim = 81*9\n",
    "\n",
    "    data_count = len(xs)\n",
    "    train_count = int(data_count * train_ratio)\n",
    "    valid_count = int(data_count * valid_ratio)\n",
    "    print('data_count', data_count)\n",
    "    print('train_count', train_count)\n",
    "    print('valid_count', valid_count)\n",
    "\n",
    "    test_start_idx = train_count + valid_count\n",
    "\n",
    "    indices = np.arange(data_count)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    self.train_xs = xs[indices[0:train_count]]\n",
    "    self.train_ys = ys[indices[0:train_count]]\n",
    "    self.validate_xs = xs[indices[train_count:test_start_idx]]\n",
    "    self.validate_ys = ys[indices[train_count:test_start_idx]]\n",
    "    self.test_xs = xs[indices[test_start_idx:]]\n",
    "    self.test_ys = ys[indices[test_start_idx:]]\n",
    "\n",
    "    self.train_count = train_count\n",
    "    \n",
    "def sudoku_demonstrate(self, x, y, estimate, answer, probs):\n",
    "    print(\"sudoku demonstrate dummy\")\n",
    "    \n",
    "SudokuDataset.__init__ = sudoku_init\n",
    "SudokuDataset.demonstrate = sudoku_demonstrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_count 50000\n",
      "train_count 40000\n",
      "valid_count 2500\n"
     ]
    }
   ],
   "source": [
    "sd = SudokuDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SudokuCnnModel(MultiLayerPerceptronModel):\n",
    "    pass\n",
    "\n",
    "def sudoku_init(self, name, dataset, hidden_dims, learning_rate=0.001):\n",
    "    MultiLayerPerceptronModel.__init__(self, name, dataset, hidden_dims, learning_rate)\n",
    "\n",
    "def sudoku_build_loss_accuracy(self):\n",
    "    #self.temp_labels = tf.reshape(self.y, [-1, 81, 9])\n",
    "    self.temp_labels = tf.cast(tf.reshape(self.y, [-1, 81]), \"int64\")\n",
    "    self.temp_logits = tf.reshape(self.output, [-1, 81, 9])\n",
    "    self.probs = tf.nn.softmax(self.temp_logits)\n",
    "    #self.temp_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=self.temp_labels, logits=self.probs)\n",
    "    self.temp_cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.temp_labels, logits=self.temp_logits)\n",
    "    self.loss = tf.reduce_mean(self.temp_cross_entropy)\n",
    "    self.temp_test = tf.argmax(self.temp_logits, 2)\n",
    "    self.estimate = tf.argmax(self.temp_logits, 2)\n",
    "    #self.answer = tf.argmax(self.temp_labels, 2)\n",
    "    self.answer = self.temp_labels\n",
    "    self.temp_correct = tf.cast(tf.equal(self.estimate, self.answer), \"float\")\n",
    "    self.accuracy = tf.reduce_mean(self.temp_correct)\n",
    "    self.max_probs = tf.reduce_max(self.probs, 2)\n",
    "    self.masked_max_probs = self.max_probs * tf.cast(1 - tf.sign(self.x), \"float\")\n",
    "    self.max_pos = tf.argmax(self.masked_max_probs, 1)\n",
    "    self.max_est = tf.argmax(self.probs, 2)\n",
    "    self.index_mask = tf.one_hot(self.max_pos, 81)\n",
    "    self.est_val = tf.reduce_sum(tf.cast(self.max_est, \"float\") * self.index_mask,1)\n",
    "    self.ans_val = tf.reduce_sum(tf.cast(self.answer, \"float\") * self.index_mask,1)\n",
    "    self.max_correct = tf.cast(tf.equal(self.est_val, self.ans_val), \"float\")\n",
    "    self.max_acc = tf.reduce_mean(self.max_correct)\n",
    "\n",
    "    #foo = tf.constant([[1,2,3], [4,5,6]])\n",
    "    #indexes = tf.constant([1,2]) #[1, 2])\n",
    "    #self.est_val = self.max_est[:, indexes]\n",
    "    #self.est_val = foo[:, indexes]\n",
    "    #self.est_val = self.max_est # self.max_est[:, self.max_pos]\n",
    "    \n",
    "    #foo = tf.constant([[1,2,3], [4,5,6]])\n",
    "    #foo[:, 1] # [2, 5]\n",
    "    #indexes = tf.constant([1, 2])\n",
    "    #foo[:, indexes] # [2, 6]\n",
    "\n",
    "\n",
    "    #self.est_val = self.max_est[self.max_pos]\n",
    "    #self.onehot = tf.cast(tf.one_hot(self.max_pos, 81), \"float\")\n",
    "    #self.max_est_val = tf.matmul(self.max_est, self.onehot) #tf.gather(self.max_est, self.max_pos)\n",
    "    #self.max_probs = tf.argmax(self.output, 2)\n",
    "    #self.est_masked = tf.multiply(self.est_max, tf.cast(1 - tf.sign(self.x), \"int64\"))\n",
    "    #self.est_max2 = tf.argmax(self.est_masked, 1)\n",
    "    #self.est_pos = tf.cast(tf.divide(self.est_max2, 10), \"int32\")\n",
    "    #self.est_val = self.est_masked % 10\n",
    "    #self.ans_val = self.est_val #tf.gather(self.answer, self.est_pos)\n",
    "    #self.max_correct = tf.cast(tf.equal(self.est_val, self.ans_val), \"float\")\n",
    "    #self.max_acc = tf.reduce_mean(self.max_correct)\n",
    "\n",
    "def sudoku_train(self, epoch_count=2, batch_size=10):\n",
    "    if batch_size == 0:\n",
    "        batch_size = self.dataset.train_count\n",
    "        \n",
    "    batch_count = int(self.dataset.train_count / batch_size)\n",
    "    report_period = epoch_count / 10\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    if random_fix: np.random.seed(1945)\n",
    "    \n",
    "    time1 = time2 = int(time.time())\n",
    "    \n",
    "    print(\"Model {} train report:\".format(self.name))\n",
    "    \n",
    "    for epoch in range(epoch_count):\n",
    "        costs = []\n",
    "        accs = []\n",
    "        max_accs = []\n",
    "        for n in range(batch_count):\n",
    "            train_X, train_Y = self.dataset.get_train_data(batch_size, n)\n",
    "            _, labels, output, logits, entropy, cost, probs, estimate, answer, correct, \\\n",
    "                max_probs, masked_max_probs, max_pos, max_est, index_mask, est_val, ans_val, max_correct, max_acc, \\\n",
    "                acc = \\\n",
    "                sess.run([self.train_op, self.temp_labels, self.output, self.temp_logits, \n",
    "                          self.temp_cross_entropy, self.loss, self.probs, self.estimate, \n",
    "                          self.answer, self.temp_correct, \n",
    "                          self.max_probs, self.masked_max_probs, self.max_pos, self.max_est, self.index_mask, self.est_val, self.ans_val,\n",
    "                          self.max_correct, self.max_acc, \n",
    "                          self.accuracy], \\\n",
    "                                    feed_dict={self.x:train_X, self.y:train_Y})\n",
    "            #_, cost, acc = sess.run([self.train_op, self.loss, self.accuracy], \\\n",
    "            #                        feed_dict={self.x:train_X, self.y:train_Y})\n",
    "            costs.append(cost)\n",
    "            accs.append(acc)\n",
    "            max_accs.append(max_acc)\n",
    "        \n",
    "        if (epoch+1) % report_period == 0:\n",
    "            validate_X, validate_Y = self.dataset.get_test_data(True, 30)\n",
    "            acc = sess.run(self.accuracy, feed_dict={self.x:validate_X, self.y:validate_Y})\n",
    "            time3 = int(time.time())\n",
    "            print(\"    Epoch {}: cost={:5.3f}, accuracy={:5.3f}, {:5.3f}/{:5.3f}, {:5.3f} ({}/{} secs)\". \\\n",
    "                  format(epoch+1, np.mean(costs), acc, np.mean(accs), max_acc, np.mean(max_accs), time3-time2, time3-time1))\n",
    "            time2 = time3\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"probs\", np.shape(probs))\n",
    "    print(probs[0])\n",
    "    print(\"max_probs\", np.shape(max_probs))\n",
    "    print(max_probs[0])\n",
    "    print(\"train_X\", np.shape(train_X))\n",
    "    print(train_X[0])\n",
    "    print(\"masked_max_probs\", np.shape(masked_max_probs))\n",
    "    print(masked_max_probs[0])\n",
    "    print(\"max_pos\", max_pos)\n",
    "    print(\"max_est\", max_est)\n",
    "    print(\"index_mask\", index_mask)\n",
    "    print(\"answer\", answer)\n",
    "    \"\"\"\n",
    "    print(\"est_val\", est_val)\n",
    "    print(\"ans_val\", ans_val)\n",
    "    print(\"max_acc\", max_acc)\n",
    "    \n",
    "    #print(\"train_X\", np.shape(train_X))\n",
    "    #print(train_X[0])\n",
    "    #print(\"train_Y\", np.shape(train_Y))\n",
    "    #print(train_Y[0])\n",
    "    #print(\"labels\", np.shape(labels))\n",
    "    #print(labels[0])\n",
    "    #print(\"output\", np.shape(output))\n",
    "    #print(output[0])\n",
    "    #print(\"logits\", np.shape(logits))\n",
    "    #print(logits[0])\n",
    "    #print(\"probs\", np.shape(probs))\n",
    "    #print(np.max(probs[0], axis=1))\n",
    "    #print(\"est_masked\", np.shape(est_masked))\n",
    "    #print(est_masked[0])\n",
    "    #print(\"est_max\", np.shape(est_max))\n",
    "    #print(est_max)\n",
    "    #print(\"est_pos\", np.shape(est_pos))\n",
    "    #print(est_pos)\n",
    "    #print(\"est_val\", np.shape(est_val))\n",
    "    #print(est_val)\n",
    "    #print(\"ans_val\", np.shape(ans_val))\n",
    "    #print(ans_val)\n",
    "    #print(\"max_correct\", np.shape(max_correct))\n",
    "    #print(max_correct)\n",
    "    #print(\"max_acc\", max_acc)\n",
    "    \"\"\"\n",
    "    print(\"train_X\", np.shape(train_X))\n",
    "    print(\"x\", np.shape(x))\n",
    "    print(x[0])\n",
    "    print(\"y\", np.shape(y))\n",
    "    print(y[0])\n",
    "    print(\"labels\", np.shape(labels))\n",
    "    print(labels[0])\n",
    "    print(\"output\", np.shape(output))\n",
    "    print(output[0])\n",
    "    print(\"logits\", np.shape(logits))\n",
    "    print(logits[0])\n",
    "    print(\"probs\", np.shape(probs))\n",
    "    print(probs[0])\n",
    "    print(\"entropy\", np.shape(entropy))\n",
    "    print(entropy[0])\n",
    "    print(\"temp_test\", np.shape(temp_test))\n",
    "    print(temp_test[0])\n",
    "    print(\"estimate\", np.shape(estimate))\n",
    "    print(estimate[0])\n",
    "    print(\"answer\", np.shape(answer))\n",
    "    print(answer[0])\n",
    "    print(\"correct\", np.shape(correct))\n",
    "    print(correct[0])\n",
    "    print(\"acc\", np.shape(acc))\n",
    "    print(acc)\n",
    "\n",
    "    print(\"\")\n",
    "    \"\"\"\n",
    "    \n",
    "    path = \"params/{}.ckpt\".format(self.name)\n",
    "    self.saver.save(sess, path)\n",
    "    sess.close()\n",
    "\n",
    "SudokuCnnModel.__init__ = sudoku_init\n",
    "SudokuCnnModel.train = sudoku_train\n",
    "SudokuCnnModel.build_loss_accuracy = sudoku_build_loss_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sudoku_build_placeholders(self):\n",
    "    MultiLayerPerceptronModel.build_placeholders(self)\n",
    "    #self.cx = tf.placeholder(\"float\", [None, 27,9,1])\n",
    "    \n",
    "    \"\"\"\n",
    "    row, col, box = np.zeros([4,7]), np.zeros([4,7]), np.zeros([4,7])\n",
    "    \n",
    "    for m in range(4):\n",
    "        for n in range(7):\n",
    "            nn = n % 4\n",
    "            row[m][n] = m * 4 + nn\n",
    "            col[m][n] = nn * 4 + m\n",
    "            box[m][n] = (m//2*4+m%2) * 2 + (nn//2*4+nn%2)\n",
    "    \n",
    "    #print('box', box)\n",
    "    f2 = np.reshape([row, col, box], [-1]).astype(int)\n",
    "                \n",
    "    self.f2 = tf.constant(f2)\n",
    "    \"\"\"\n",
    "    \n",
    "    row, col, box = np.zeros([9,17]), np.zeros([9,17]), np.zeros([9,17])\n",
    "    \n",
    "    for m in range(9):\n",
    "        for n in range(17):\n",
    "            nn = n % 9\n",
    "            row[m][n] = m * 9 + nn\n",
    "            col[m][n] = nn * 9 + m\n",
    "            box[m][n] = (m//3*9+m%3) * 3 + (nn//3*9+nn%3)\n",
    "    \n",
    "    #print('box', box)\n",
    "    f3 = np.reshape([row, col, box], [-1]).astype(int)\n",
    "                \n",
    "    self.f3 = tf.constant(f3)\n",
    "\n",
    "    rev_map = np.zeros([9,9,3])\n",
    "    \n",
    "    for r in range(9):\n",
    "        for c in range(9):\n",
    "            rev_map[r,c,0] = r * 9 + c\n",
    "            rev_map[r,c,1] = 81 + c * 9 + r\n",
    "            rev_map[r,c,2] = 162 + ((r//3)*3+c//3) * 9 + (r%3)*3+c%3\n",
    "\n",
    "    #print(\"rev_map\", rev_map);\n",
    "                    \n",
    "    frev3 = np.reshape(rev_map, [-1]).astype(int)\n",
    "                \n",
    "    self.ff3 = tf.constant(frev3)\n",
    "\n",
    "    \"\"\"\n",
    "    #cube2, cube3 = np.zeros([2,4,4]), np.zeros([2,9,9])\n",
    "    cube3 = np.zeros([2,4,4])\n",
    "\n",
    "    for k in range(2):\n",
    "        for m in range(4):\n",
    "            for n in range(4):\n",
    "                cube2[k][m][n] = (k+1)*100+(m+1)*10+(n+1)\n",
    "    \n",
    "    for k in range(2):\n",
    "        for m in range(9):\n",
    "            for n in range(9):\n",
    "                cube3[k][m][n] = (k+1)*100+(m+1)*10+(n+1)\n",
    "                \n",
    "    self.x2 = tf.constant(np.reshape(cube2, [2,16]).astype(np.float32))\n",
    "    self.x3 = tf.constant(np.reshape(cube3, [2,81]).astype(np.float32))\n",
    "\n",
    "    #self.f2 = tf.constant([[0,4,8,12,1,5,9,13,2,6,10,14,3,7,11,15]])\n",
    "    \n",
    "    self.g2 = tf.reshape(tf.gather(self.x2, self.f2, axis=1), [-1, 12, 4+3, 1])\n",
    "    \"\"\"\n",
    "\n",
    "def sudoku_build_parameters(self, hidden_dim):\n",
    "    MultiLayerPerceptronModel.build_parameters(self, [hidden_dim])\n",
    "    #self.kernel_xw = tf.Variable(tf.ones([1, 9, 1, hidden_dim], tf.float32)) #tf.random_normal([1,9,1,hidden_dim], stddev=0.03))\n",
    "    #self.kernel_xb = tf.Variable(tf.zeros([hidden_dim]))\n",
    "    #w = np.zeros([1, 9, 1, hidden_dim], dtype=\"float32\")\n",
    "    #w[0,0,0,0] = 1\n",
    "    #w[0,:,0,1] = 1\n",
    "    self.kernel_xw = tf.Variable(tf.random_normal([1,9,1,hidden_dim], stddev=0.03))\n",
    "    self.kernel_xb = tf.Variable(tf.zeros([hidden_dim]))\n",
    "\n",
    "    #w2 = np.zeros([1, 1, 3*hidden_dim, 9], dtype=\"float32\")\n",
    "    #w2[0,0,0,0] = 1\n",
    "    #w2[0,:,0,1] = 1\n",
    "    self.kernel_xw2 = tf.Variable(tf.random_normal([1, 1, 3*hidden_dim, 9], stddev=0.03))\n",
    "    self.kernel_xb2 = tf.Variable(tf.zeros([9]))\n",
    "\n",
    "def sudoku_build_neuralnet(self, hidden_dim):\n",
    "    MultiLayerPerceptronModel.build_neuralnet(self, [hidden_dim])\n",
    "    \n",
    "    self.g3 = tf.reshape(tf.gather(self.x, self.f3, axis=1), [-1, 27, 9+8, 1])\n",
    "\n",
    "    #conv_linear = tf.nn.conv2d(self.g3, self.kernel_xw, strides=[1, 1, 9, 1], padding='SAME', data_format='NHWC')\n",
    "    conv_linear = tf.nn.conv2d(self.g3, self.kernel_xw, strides=[1, 1, 1, 1], padding='VALID', data_format='NHWC')\n",
    "    conv_with_b = tf.nn.bias_add(conv_linear, self.kernel_xb, data_format='NHWC')\n",
    "    conv_out = tf.reshape(tf.nn.relu(conv_with_b), [-1,243,hidden_dim])\n",
    "    self.conv2_in = tf.reshape(tf.gather(conv_out, self.ff3, axis=1), [-1, 81, 1, 3*hidden_dim])\n",
    "    \n",
    "    conv_linear2 = tf.nn.conv2d(self.conv2_in, self.kernel_xw2, strides=[1, 1, 1, 1], padding='VALID', data_format='NHWC')\n",
    "    conv_with_b2 = tf.nn.bias_add(conv_linear2, self.kernel_xb2, data_format='NHWC')\n",
    "    conv_out2 = tf.nn.relu(conv_with_b2)\n",
    "\n",
    "    self.conv_out = tf.reshape(tf.nn.relu(conv_out2), [-1, 81, 9])\n",
    "    self.output = self.conv_out\n",
    "\n",
    "SudokuCnnModel.build_placeholders = sudoku_build_placeholders\n",
    "SudokuCnnModel.build_parameters = sudoku_build_parameters\n",
    "SudokuCnnModel.build_neuralnet = sudoku_build_neuralnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sudoku_dump(self, batch_size=100):\n",
    "    sess = tf.Session()\n",
    "    path = \"params/{}.ckpt\".format(self.name)\n",
    "    self.saver.restore(sess, path)\n",
    "    \n",
    "    train_X, train_Y = self.dataset.get_train_data(batch_size, 0)\n",
    "    \n",
    "    loss, est_val, ans_val, max_acc, acc, conv2_in, conv_out = \\\n",
    "        sess.run([self.loss, self.est_val, self.ans_val, self.max_acc, self.accuracy, self.conv2_in, self.conv_out], \\\n",
    "                 feed_dict={self.x:train_X, self.y:train_Y})\n",
    "\n",
    "    print(\"est_val\", est_val)\n",
    "    print(\"ans_val\", ans_val)\n",
    "    print(\"max_acc\", max_acc)\n",
    "    \n",
    "    print('loss', loss)\n",
    "\n",
    "    print('done')\n",
    "    \n",
    "    sess.close()\n",
    "\n",
    "SudokuCnnModel.dump = sudoku_dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from params/sudoku-cnn-1.ckpt\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Key sudoku-cnn-1_21/Variable_3 not found in checkpoint\n\t [[Node: sudoku-cnn-1_21/save/RestoreV2_115 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_sudoku-cnn-1_21/save/Const_0_0, sudoku-cnn-1_21/save/RestoreV2_115/tensor_names, sudoku-cnn-1_21/save/RestoreV2_115/shape_and_slices)]]\n\t [[Node: sudoku-cnn-1_21/save/RestoreV2_142/_211 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_534_sudoku-cnn-1_21/save/RestoreV2_142\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\nCaused by op 'sudoku-cnn-1_21/save/RestoreV2_115', defined at:\n  File \"/usr/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.4/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.4/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.4/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.4/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.4/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.4/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.4/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.4/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.4/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.4/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-111-8eea6e4cb332>\", line 1, in <module>\n    sm1 = SudokuCnnModel(\"sudoku-cnn-1\", sd, 2)\n  File \"<ipython-input-108-53353353135a>\", line 5, in sudoku_init\n    MultiLayerPerceptronModel.__init__(self, name, dataset, hidden_dims, learning_rate)\n  File \"<ipython-input-15-ee230eede2ff>\", line 9, in mlp_init\n    build_saver(self)\n  File \"<ipython-input-20-3c5344e83047>\", line 7, in build_saver\n    self.saver = tf.train.Saver(var_list=var_list)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/saver.py\", line 1218, in __init__\n    self.build()\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/saver.py\", line 1227, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/saver.py\", line 1263, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/saver.py\", line 751, in _build_internal\n    restore_sequentially, reshape)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/saver.py\", line 427, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/saver.py\", line 267, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 1021, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): Key sudoku-cnn-1_21/Variable_3 not found in checkpoint\n\t [[Node: sudoku-cnn-1_21/save/RestoreV2_115 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_sudoku-cnn-1_21/save/Const_0_0, sudoku-cnn-1_21/save/RestoreV2_115/tensor_names, sudoku-cnn-1_21/save/RestoreV2_115/shape_and_slices)]]\n\t [[Node: sudoku-cnn-1_21/save/RestoreV2_142/_211 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_534_sudoku-cnn-1_21/save/RestoreV2_142\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key sudoku-cnn-1_21/Variable_3 not found in checkpoint\n\t [[Node: sudoku-cnn-1_21/save/RestoreV2_115 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_sudoku-cnn-1_21/save/Const_0_0, sudoku-cnn-1_21/save/RestoreV2_115/tensor_names, sudoku-cnn-1_21/save/RestoreV2_115/shape_and_slices)]]\n\t [[Node: sudoku-cnn-1_21/save/RestoreV2_142/_211 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_534_sudoku-cnn-1_21/save/RestoreV2_142\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-8eea6e4cb332>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msm1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSudokuCnnModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sudoku-cnn-1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msm1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-110-ddc4b5b84dc7>\u001b[0m in \u001b[0;36msudoku_dump\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"params/{}.ckpt\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1664\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_graph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1665\u001b[0m       sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1666\u001b[0;31m                {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1667\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key sudoku-cnn-1_21/Variable_3 not found in checkpoint\n\t [[Node: sudoku-cnn-1_21/save/RestoreV2_115 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_sudoku-cnn-1_21/save/Const_0_0, sudoku-cnn-1_21/save/RestoreV2_115/tensor_names, sudoku-cnn-1_21/save/RestoreV2_115/shape_and_slices)]]\n\t [[Node: sudoku-cnn-1_21/save/RestoreV2_142/_211 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_534_sudoku-cnn-1_21/save/RestoreV2_142\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\nCaused by op 'sudoku-cnn-1_21/save/RestoreV2_115', defined at:\n  File \"/usr/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.4/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.4/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.4/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.4/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.4/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.4/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.4/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.4/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.4/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.4/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-111-8eea6e4cb332>\", line 1, in <module>\n    sm1 = SudokuCnnModel(\"sudoku-cnn-1\", sd, 2)\n  File \"<ipython-input-108-53353353135a>\", line 5, in sudoku_init\n    MultiLayerPerceptronModel.__init__(self, name, dataset, hidden_dims, learning_rate)\n  File \"<ipython-input-15-ee230eede2ff>\", line 9, in mlp_init\n    build_saver(self)\n  File \"<ipython-input-20-3c5344e83047>\", line 7, in build_saver\n    self.saver = tf.train.Saver(var_list=var_list)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/saver.py\", line 1218, in __init__\n    self.build()\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/saver.py\", line 1227, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/saver.py\", line 1263, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/saver.py\", line 751, in _build_internal\n    restore_sequentially, reshape)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/saver.py\", line 427, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/saver.py\", line 267, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 1021, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): Key sudoku-cnn-1_21/Variable_3 not found in checkpoint\n\t [[Node: sudoku-cnn-1_21/save/RestoreV2_115 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_sudoku-cnn-1_21/save/Const_0_0, sudoku-cnn-1_21/save/RestoreV2_115/tensor_names, sudoku-cnn-1_21/save/RestoreV2_115/shape_and_slices)]]\n\t [[Node: sudoku-cnn-1_21/save/RestoreV2_142/_211 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_534_sudoku-cnn-1_21/save/RestoreV2_142\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "sm1 = SudokuCnnModel(\"sudoku-cnn-1\", sd, 2)\n",
    "sm1.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sudoku-cnn-1 train report:\n",
      "    Epoch 1: cost=2.196, accuracy=0.118, 0.113/0.140, 0.106 (2/2 secs)\n",
      "    Epoch 2: cost=2.194, accuracy=0.138, 0.123/0.070, 0.110 (3/5 secs)\n",
      "    Epoch 3: cost=2.191, accuracy=0.169, 0.151/0.170, 0.141 (2/7 secs)\n",
      "    Epoch 4: cost=2.187, accuracy=0.195, 0.178/0.140, 0.175 (2/9 secs)\n",
      "    Epoch 5: cost=2.183, accuracy=0.177, 0.184/0.180, 0.196 (3/12 secs)\n",
      "    Epoch 6: cost=2.176, accuracy=0.189, 0.183/0.190, 0.212 (2/14 secs)\n",
      "    Epoch 7: cost=2.162, accuracy=0.189, 0.185/0.170, 0.225 (2/16 secs)\n",
      "    Epoch 8: cost=2.143, accuracy=0.188, 0.187/0.220, 0.239 (3/19 secs)\n",
      "    Epoch 9: cost=2.121, accuracy=0.180, 0.186/0.230, 0.250 (2/21 secs)\n",
      "    Epoch 10: cost=2.094, accuracy=0.177, 0.180/0.250, 0.258 (2/23 secs)\n",
      "est_val [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "ans_val [ 0.  2.  0.  3.  0.  8.  5.  4.  5.  0.  3.  0.  6.  4.  3.  6.  1.  2.\n",
      "  1.  4.  5.  0.  1.  1.  1.  1.  5.  5.  0.  4.  4.  4.  0.  3.  2.  2.\n",
      "  0.  1.  1.  1.  2.  3.  1.  3.  0.  4.  0.  3.  0.  0.  5.  3.  4.  4.\n",
      "  7.  0.  0.  0.  1.  2.  1.  1.  4.  0.  2.  6.  1.  7.  1.  0.  1.  2.\n",
      "  2.  0.  6.  4.  4.  1.  3.  2.  6.  0.  0.  0.  4.  1.  6.  0.  3.  2.\n",
      "  5.  5.  3.  0.  6.  4.  5.  1.  1.  0.]\n",
      "max_acc 0.25\n"
     ]
    }
   ],
   "source": [
    "sm1 = SudokuCnnModel(\"sudoku-cnn-1\", sd, 5)\n",
    "sm1.train(epoch_count=10, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sudoku-cnn-1 train report:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (100, 81) for Tensor 'sudoku-cnn-1_22/Placeholder_1:0', which has shape '(?, 729)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-1e6c80960c23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msm1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSudokuCnnModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sudoku-cnn-1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msm1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-108-53353353135a>\u001b[0m in \u001b[0;36msudoku_train\u001b[0;34m(self, epoch_count, batch_size)\u001b[0m\n\u001b[1;32m     81\u001b[0m                           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_correct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                           self.accuracy], \\\n\u001b[0;32m---> 83\u001b[0;31m                                     feed_dict={self.x:train_X, self.y:train_Y})\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0;31m#_, cost, acc = sess.run([self.train_op, self.loss, self.accuracy], \\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;31m#                        feed_dict={self.x:train_X, self.y:train_Y})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1094\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1097\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (100, 81) for Tensor 'sudoku-cnn-1_22/Placeholder_1:0', which has shape '(?, 729)'"
     ]
    }
   ],
   "source": [
    "sm1 = SudokuCnnModel(\"sudoku-cnn-1\", sd, 5)\n",
    "sm1.train(epoch_count=10, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sudoku-cnn-2 train report:\n",
      "    Epoch 10: cost=0.963, accuracy=0.660, 0.653/0.300, 0.315 (183/183 secs)\n",
      "    Epoch 20: cost=0.877, accuracy=0.655, 0.657/0.100, 0.335 (175/358 secs)\n",
      "    Epoch 30: cost=0.863, accuracy=0.656, 0.660/0.100, 0.350 (177/535 secs)\n",
      "    Epoch 40: cost=0.854, accuracy=0.658, 0.663/0.300, 0.362 (177/712 secs)\n",
      "    Epoch 50: cost=0.842, accuracy=0.668, 0.669/0.300, 0.402 (177/889 secs)\n",
      "    Epoch 60: cost=0.832, accuracy=0.671, 0.673/0.600, 0.440 (176/1065 secs)\n",
      "    Epoch 70: cost=0.822, accuracy=0.676, 0.677/0.400, 0.466 (175/1240 secs)\n",
      "    Epoch 80: cost=0.816, accuracy=0.679, 0.678/0.500, 0.473 (174/1414 secs)\n",
      "    Epoch 90: cost=0.812, accuracy=0.681, 0.680/0.400, 0.501 (174/1588 secs)\n",
      "    Epoch 100: cost=0.809, accuracy=0.682, 0.682/0.500, 0.532 (172/1760 secs)\n",
      "est_val [ 0.  1.  1.  0.  1.  3.  1.  1.  1.  1.]\n",
      "ans_val [ 1.  1.  1.  0.  6.  2.  1.  0.  1.  2.]\n",
      "max_acc 0.5\n"
     ]
    }
   ],
   "source": [
    "sm2 = SudokuCnnModel(\"sudoku-cnn-2\", sd, 64, learning_rate=0.01)\n",
    "sm2.train(epoch_count=100, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from params/sudoku-cnn-2.ckpt\n",
      "est_val [ 7.  0.  1.  1.  1.  1.  1.  0.  0.  1.  0.  1.  0.  0.  1.  1.  1.  1.\n",
      "  2.  1.  4.  1.  0.  0.  1.  1.  0.  1.  1.  1.  0.  1.  1.  1.  7.  1.\n",
      "  1.  7.  7.  1.  4.  1.  0.  3.  0.  1.  1.  4.  0.  1.  1.  1.  1.  0.\n",
      "  7.  1.  1.  4.  3.  4.  1.  3.  7.  1.  1.  1.  0.  1.  1.  0.  1.  4.\n",
      "  1.  0.  1.  1.  1.  1.  1.  1.  4.  0.  1.  3.  1.  0.  1.  3.  7.  4.\n",
      "  4.  0.  1.  0.  0.  0.  1.  1.  1.  4.]\n",
      "ans_val [ 8.  0.  0.  1.  1.  1.  1.  1.  2.  1.  1.  0.  1.  1.  0.  1.  7.  1.\n",
      "  0.  1.  4.  0.  1.  1.  2.  1.  0.  2.  1.  1.  1.  7.  1.  1.  6.  2.\n",
      "  1.  8.  8.  1.  5.  1.  2.  3.  6.  0.  1.  4.  1.  2.  3.  2.  2.  0.\n",
      "  7.  1.  3.  3.  4.  4.  1.  3.  7.  1.  1.  0.  1.  3.  0.  1.  1.  5.\n",
      "  2.  1.  1.  0.  4.  0.  0.  2.  5.  0.  1.  2.  1.  1.  0.  3.  6.  4.\n",
      "  5.  0.  1.  1.  0.  2.  1.  1.  2.  4.]\n",
      "max_acc 0.44\n",
      "loss 0.806386\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "sm2.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from params/sudoku-cnn-2.ckpt\n",
      "Model sudoku-cnn-2 test report: accuracy = 0.441, (0 secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print(sm1)\n",
    "sm2.test()\n",
    "#m2.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sudoku_demonstrate(self, x, y, estimate, answer, probs):\n",
    "    unknown_cnt = 0\n",
    "    known_mismatch_cnt = 0\n",
    "    \n",
    "    for n in range(81):\n",
    "        if x[n] != 0:\n",
    "            est = np.argmax(probs[n])\n",
    "            if est != x[n]: known_mismatch_cnt += 1\n",
    "            probs[n,:] = 0\n",
    "        else: unknown_cnt += 1\n",
    "            \n",
    "    print('unknown_cnt', unknown_cnt)\n",
    "    print('known_mismatch_cnt', known_mismatch_cnt)\n",
    "    idx = np.argmax(probs)\n",
    "    print(\"shape(probs)\", np.shape(probs))\n",
    "    print(\"idx(argmax)\", idx)\n",
    "    \"\"\"\n",
    "    pos = int(idx / 10)\n",
    "    val = idx % 10\n",
    "    ans = np.argmax(y[pos*10:(pos+1)*10])\n",
    "    #print(idx, pos, val)\n",
    "    print(x[pos], val, ans, probs[pos][val])\n",
    "    print(probs[pos])\n",
    "    \"\"\"\n",
    "    \n",
    "SudokuDataset.demonstrate = sudoku_demonstrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from params/sudoku-cnn-2.ckpt\n",
      "Model sudoku-cnn-2 Demonstration\n",
      "unknown_cnt 27\n",
      "known_mismatch_cnt 45\n",
      "shape(probs) (81, 9)\n",
      "idx(argmax) 378\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sm2.demonstrate(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
