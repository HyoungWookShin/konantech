{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_fix = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, mode):\n",
    "        self.mode = mode\n",
    "        self.input_dim = self.output_dim = self.train_count = 0\n",
    "        self.train_xs = self.test_xs = self.validate_xs = []\n",
    "        self.train_ys = self.test_ys = self.validate_ys = []\n",
    "        self.target_names = []\n",
    "    \n",
    "    def get_train_data(self, batch_size, nth):\n",
    "        if nth == 0:\n",
    "            self.indices = np.arange(self.train_count)\n",
    "            np.random.shuffle(self.indices)\n",
    "        \n",
    "        from_idx = nth * batch_size\n",
    "        to_idx = (nth + 1) * batch_size\n",
    "    \n",
    "        train_X = self.train_xs[self.indices[from_idx:to_idx]]\n",
    "        train_Y = self.train_ys[self.indices[from_idx:to_idx]]\n",
    "        \n",
    "        return train_X, train_Y    \n",
    "    \n",
    "    def get_test_data(self, validate=False, count=0):\n",
    "        if validate:\n",
    "            xs, ys = self.validate_xs, self.validate_ys\n",
    "        else:\n",
    "            xs, ys = self.test_xs, self.test_ys\n",
    "        \n",
    "        if count == 0:\n",
    "            return xs, ys\n",
    "        \n",
    "        if count > len(xs): count = len(xs)\n",
    "        \n",
    "        indices = np.arange(len(xs))\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        return xs[indices[0:count]], ys[indices[0:count]]\n",
    "    \n",
    "    def get_target_name(self, idxs):\n",
    "        return self.target_names[idxs]\n",
    "    \n",
    "    def demonstrate(self, x, estimate, answer):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "class IrisDataset(Dataset):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iris_init(self, mode, train_ratio=0.8):\n",
    "    Dataset.__init__(self, mode)\n",
    "    \n",
    "    dataset = datasets.load_iris()\n",
    "    \n",
    "    xs, ys = iris_prepare(self, mode, dataset.data, dataset.target)\n",
    "    \n",
    "    data_count = len(dataset.data)\n",
    "    self.train_count = int(data_count * train_ratio)\n",
    "\n",
    "    if random_fix: np.random.seed(1234)\n",
    "\n",
    "    indices = np.arange(data_count)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    self.train_xs = xs[indices[0:self.train_count]]\n",
    "    self.train_ys = ys[indices[0:self.train_count]]\n",
    "    self.test_xs = self.validate_xs = xs[indices[self.train_count:]]\n",
    "    self.test_ys = self.validate_ys = ys[indices[self.train_count:]]\n",
    "    \n",
    "    self.target_names = dataset.target_names\n",
    "\n",
    "def iris_prepare(self, mode, data, target):\n",
    "    if mode == \"regression\":\n",
    "        self.input_dim = 3\n",
    "        self.output_dim = 1\n",
    "        xs = data[:, 0:3]\n",
    "        ys = data[:, 3:4]\n",
    "    elif mode == \"binary\":\n",
    "        self.input_dim = 4\n",
    "        self.output_dim = 1\n",
    "        xs = data\n",
    "        ys = np.equal(target, 0).astype(\"float32\").reshape(-1,1)\n",
    "    elif mode == \"select\":\n",
    "        self.input_dim = 4\n",
    "        self.output_dim = 3\n",
    "        xs = data\n",
    "        ys = np.eye(3)[target]\n",
    "        \n",
    "    return xs, ys\n",
    "\n",
    "IrisDataset.__init__ = iris_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iris_demonstrate(self, x, estimate, answer):\n",
    "    if self.mode == \"regression\":\n",
    "        print(\"({}, {}, {}) => 추정 {:3.1f}, 정답: {:3.1f}\".format(x[0], x[1], x[2], estimate, answer))\n",
    "    elif self.mode == \"binary\":\n",
    "        estr = \"is setosa\"\n",
    "        astr = \"(오답)\"\n",
    "        if not estimate: estr = \"is not setosa\"\n",
    "        if estimate == answer: astr = \"(정답)\"\n",
    "        print(\"({}, {}, {}, {}) => {} {}\".format(x[0], x[1], x[2], x[3], estr, astr))\n",
    "    elif self.mode == \"select\":\n",
    "        estr = self.target_names[estimate]\n",
    "        astr = \"({})\".format(self.target_names[answer])\n",
    "        if estimate == answer: astr = \"(정답)\"\n",
    "        print(\"({}, {}, {}, {}) => {} {}\".format(x[0], x[1], x[2], x[3], estr, astr))\n",
    "    \n",
    "IrisDataset.demonstrate = iris_demonstrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "id1 = IrisDataset(\"regression\")\n",
    "id2 = IrisDataset(\"binary\", 0.88)\n",
    "id3 = IrisDataset(\"select\", 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MnistDataset(Dataset):\n",
    "    pass\n",
    "\n",
    "def mnist_init(self):\n",
    "    Dataset.__init__(self, \"select\")\n",
    "    \n",
    "    dataset = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "        \n",
    "    self.train_xs = dataset.train.images\n",
    "    self.train_ys = dataset.train.labels\n",
    "        \n",
    "    self.validate_xs = dataset.validation.images\n",
    "    self.validate_ys = dataset.validation.labels\n",
    "        \n",
    "    self.test_xs = dataset.test.images\n",
    "    self.test_ys = dataset.test.labels\n",
    "\n",
    "    self.input_dim = 28 * 28\n",
    "    self.output_dim = 10\n",
    "    \n",
    "    self.train_count = len(dataset.train.images)\n",
    "    self.target_names = ['0','1','2','3','4','5','6','7','8','9']\n",
    "\n",
    "def mnist_demonstrate(self, xs, est, ans):\n",
    "    rows, cols = 4, 4\n",
    "    f, a = plt.subplots(rows, cols, figsize=(cols, rows))\n",
    "    for row in range(cols):\n",
    "        for col in range(cols):\n",
    "            i = row * cols + col\n",
    "            estr = self.target_names[est[i]]\n",
    "            astr = self.target_names[ans[i]]\n",
    "            if est[i] == ans[i]:\n",
    "                caption = \"{}\".format(estr)\n",
    "            else:\n",
    "                caption = \"{}=>{}\".format(astr, estr)\n",
    "            a[row][col].axvspan(0, 0, 0, 6.0)\n",
    "            a[row][col].imshow(np.reshape(xs[i], (28,28)))\n",
    "            a[row][col].text(0.5, -1.5, caption)\n",
    "            a[row][col].axis('off')\n",
    "    f.show()\n",
    "    plt.draw()\n",
    "    plt.show()\n",
    "        \n",
    "MnistDataset.__init__ = mnist_init\n",
    "MnistDataset.demonstrate = mnist_demonstrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "md = MnistDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, name, dataset):\n",
    "        self.name = name\n",
    "        self.dataset = dataset\n",
    "        \n",
    "    def train(self, epoch_count=10, batch_size=10):\n",
    "        pass\n",
    "    \n",
    "    def test(self):\n",
    "        pass\n",
    "    \n",
    "    def demonstrate(self, num=10, batch=False):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class MultiLayerPerceptronModel(Model):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def mlp_model_train(self, epoch_count=10, batch_size=10):\n",
    "    if batch_size == 0:\n",
    "        batch_size = self.dataset.train_count\n",
    "        \n",
    "    batch_count = int(self.dataset.train_count / batch_size)\n",
    "    report_period = epoch_count / 10\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    if random_fix: np.random.seed(1945)\n",
    "    \n",
    "    time1 = time2 = int(time.time())\n",
    "    \n",
    "    print(\"Model {} train report:\".format(self.name))\n",
    "    \n",
    "    dset = self.dataset\n",
    "    run_targets = [self.train_op, self.loss, self.accuracy]\n",
    "\n",
    "    for epoch in range(epoch_count):\n",
    "        costs = []\n",
    "        accs = []\n",
    "        for n in range(batch_count):\n",
    "            train_X, train_Y = dset.get_train_data(batch_size, n)\n",
    "            _, cost, acc = sess.run(run_targets, \\\n",
    "                feed_dict={self.x:train_X, self.y:train_Y})\n",
    "            costs.append(cost)\n",
    "            accs.append(acc)\n",
    "            \n",
    "        if (epoch+1) % report_period == 0:\n",
    "            validate_X, validate_Y = dset.get_test_data(True, 30)\n",
    "            acc = sess.run(self.accuracy, \\\n",
    "                feed_dict={self.x:validate_X, self.y:validate_Y})\n",
    "            time3 = int(time.time())\n",
    "            print(\"    Epoch {}: cost={:5.3f}, \\\n",
    "accuracy={:5.3f}/{:5.3f} ({}/{} secs)\". \\\n",
    "                  format(epoch+1, np.mean(costs), np.mean(accs), \\\n",
    "                         acc, time3-time2, time3-time1))\n",
    "            time2 = time3\n",
    "\n",
    "    print(\"\")\n",
    "    \n",
    "    path = \"params/{}.ckpt\".format(self.name)\n",
    "    self.saver.save(sess, path)\n",
    "    sess.close()\n",
    "\n",
    "MultiLayerPerceptronModel.train = mlp_model_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model_test(self):\n",
    "    test_X, test_Y = self.dataset.get_test_data()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    path = \"params/{}.ckpt\".format(self.name)\n",
    "    self.saver.restore(sess, path)\n",
    "    \n",
    "    time1 = int(time.time())\n",
    "    acc = sess.run(self.accuracy, feed_dict={self.x:test_X, self.y:test_Y})\n",
    "    time2 = int(time.time())\n",
    "    \n",
    "    print(\"Model {} test report: accuracy = {:5.3f}, ({} secs)\".format(self.name, acc, time2-time1))\n",
    "    print(\"\")\n",
    "    \n",
    "    sess.close()\n",
    "\n",
    "MultiLayerPerceptronModel.test = mlp_model_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model_demonstrate(self, num=10, batch=False):\n",
    "    demo_X, demo_Y = self.dataset.get_test_data(False, num)\n",
    "\n",
    "    sess = tf.Session()\n",
    "    path = \"params/{}.ckpt\".format(self.name)\n",
    "    self.saver.restore(sess, path)\n",
    "    \n",
    "    print(\"Model {} Demonstration\".format(self.name))\n",
    "    est, ans, probs = sess.run([self.estimate, self.answer, self.probs], \\\n",
    "           feed_dict={self.x:demo_X, self.y:demo_Y})\n",
    "    if batch:\n",
    "        self.dataset.demonstrate(demo_X, demo_Y, est, ans, probs)\n",
    "    else:\n",
    "        for n in range(len(demo_X)):\n",
    "            self.dataset.demonstrate(demo_X[n], demo_Y[n], est[n], ans[n], probs[n])\n",
    "        print(\"\")\n",
    "    \n",
    "    sess.close()\n",
    "    \n",
    "MultiLayerPerceptronModel.demonstrate = mlp_model_demonstrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_init(self, name, dataset, hidden_dims, learning_rate=0.001):\n",
    "    Model.__init__(self, name, dataset)\n",
    "    with tf.variable_scope(self.name):\n",
    "        self.build_placeholders()\n",
    "        self.build_parameters(hidden_dims)\n",
    "        self.build_neuralnet(hidden_dims)\n",
    "        self.build_loss_accuracy()\n",
    "        build_optimizer(self, learning_rate)\n",
    "        build_saver(self)\n",
    "\n",
    "MultiLayerPerceptronModel.__init__ = mlp_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_placeholders(self):\n",
    "    input_dim = self.dataset.input_dim\n",
    "    output_dim = self.dataset.output_dim\n",
    "    self.x = tf.placeholder(\"float\", [None, input_dim])\n",
    "    self.y = tf.placeholder(\"float\", [None, output_dim])\n",
    "\n",
    "MultiLayerPerceptronModel.build_placeholders = build_placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_parameters(self, hidden_dims):\n",
    "    if random_fix: np.random.seed(9876)\n",
    "\n",
    "    input_dim = self.dataset.input_dim\n",
    "    output_dim = self.dataset.output_dim\n",
    "\n",
    "    self.w_hids, self.b_hids = [], []\n",
    "    \n",
    "    prev_dim = input_dim\n",
    "\n",
    "    for n in range(len(hidden_dims)):\n",
    "        next_dim = hidden_dims[n]\n",
    "        w = tf.Variable(init_rand_normal(prev_dim, next_dim))\n",
    "        b = tf.Variable(tf.zeros([next_dim]))\n",
    "        self.w_hids.append(w)\n",
    "        self.b_hids.append(b)\n",
    "        prev_dim = next_dim\n",
    "\n",
    "    self.w_out = tf.Variable(init_rand_normal(prev_dim, output_dim))\n",
    "    self.b_out = tf.Variable(tf.zeros([output_dim]))\n",
    "    \n",
    "def init_rand_normal(in_dim, out_dim, rand_std=0.0300):\n",
    "    if not random_fix:\n",
    "        init = tf.random_normal([in_dim, out_dim], stddev=rand_std)\n",
    "    else:\n",
    "        init_64 = np.random.normal(0, rand_std, [in_dim, out_dim])\n",
    "        init = init_64.astype('float32')\n",
    "\n",
    "    return init\n",
    "        \n",
    "MultiLayerPerceptronModel.build_parameters = build_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_neuralnet(self, hidden_dims):\n",
    "    hidden = self.x\n",
    "    for n in range(len(hidden_dims)):\n",
    "        affine = tf.matmul(hidden, self.w_hids[n]) + self.b_hids[n]\n",
    "        hidden = tf.nn.relu(affine)\n",
    "    self.output = tf.matmul(hidden, self.w_out) + self.b_out\n",
    "        \n",
    "MultiLayerPerceptronModel.build_neuralnet = build_neuralnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loss_accuracy(self):\n",
    "    if self.dataset.mode == \"regression\":\n",
    "        self.estimate = self.output[:,0]\n",
    "        self.answer = self.y[:,0]\n",
    "        diff = self.estimate - self.answer\n",
    "        self.probs = tf.constant(0)\n",
    "        self.loss = tf.reduce_mean(tf.pow(diff, 2))\n",
    "        error = tf.reduce_mean(tf.abs(diff) / self.answer)\n",
    "        self.accuracy = 1 - error\n",
    "    elif self.dataset.mode == \"binary\":\n",
    "        cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.y, logits=self.output)\n",
    "        self.loss = tf.reduce_mean(cross_entropy)\n",
    "        self.probs = tf.nn.sigmoid(output)\n",
    "        self.estimate = tf.greater(self.output, 0)\n",
    "        self.answer = tf.equal(self.y, 1.0)\n",
    "        correct_bool = tf.equal(self.estimate, self.answer)\n",
    "        correct = tf.cast(correct_bool, \"float\")\n",
    "        self.accuracy = tf.reduce_mean(correct)\n",
    "    elif self.dataset.mode == \"select\":\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=self.y, logits=self.output)\n",
    "        self.loss = tf.reduce_mean(cross_entropy)\n",
    "        self.probs = tf.nn.softmax(output)\n",
    "        self.estimate = tf.argmax(self.output, 1)\n",
    "        self.answer = tf.argmax(self.y, 1)\n",
    "        correct_bool = tf.equal(self.estimate, self.answer)\n",
    "        correct = tf.cast(correct_bool, \"float\")\n",
    "        self.accuracy = tf.reduce_mean(correct)\n",
    "        \n",
    "MultiLayerPerceptronModel.build_loss_accuracy = build_loss_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(self, learning_rate):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    self.train_op = optimizer.minimize(self.loss)\n",
    "\n",
    "def build_saver(self):\n",
    "    var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
    "    self.saver = tf.train.Saver(var_list=var_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def relu_derv(y):\n",
    "    return np.sign(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derv(x, y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "def sigmoid_cross_entropy(z, x):\n",
    "    return np.maximum(x, 0) - x * z + np.log(1 + np.exp(-np.abs(x)))\n",
    "\n",
    "def sigmoid_cross_entropy_derv(z, x):\n",
    "    return -z + sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    max_elem = np.max(x, axis=1)\n",
    "    diff = (x.transpose() - max_elem).transpose()\n",
    "    exp = np.exp(diff)\n",
    "    sum_exp = np.sum(exp, axis=1)\n",
    "    probs = (exp.transpose() / sum_exp).transpose()\n",
    "    return probs\n",
    "\n",
    "def softmax_derv(x, y):\n",
    "    mb_size, nom_size = x.shape\n",
    "    derv = np.ndarray([mb_size, nom_size, nom_size])\n",
    "    for n in range(mb_size):\n",
    "        for i in range(nom_size):\n",
    "            for j in range(nom_size):\n",
    "                derv[n, i, j] = -y[n,i] * y[n,j]\n",
    "            derv[n, i, i] += y[n,i]\n",
    "    return derv\n",
    "\n",
    "def softmax_cross_entropy(p, q):\n",
    "    return -np.sum(p * np.log(q), axis=1)\n",
    "\n",
    "def softmax_cross_entropy_derv(p, q):\n",
    "    return -p / q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoTFMLPModel(Model):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notf_init(self,name,dataset,hidden_dims,learning_rate=0.001):\n",
    "    Model.__init__(self, name, dataset)\n",
    "    init_parameters(self, hidden_dims)\n",
    "    self.learning_rate = learning_rate\n",
    "    self.hidden_dims = hidden_dims\n",
    "    \n",
    "NoTFMLPModel.__init__ = notf_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def notf_model_train(self, epoch_count=10, batch_size=10):\n",
    "    if batch_size == 0:\n",
    "        batch_size = self.dataset.train_count\n",
    "        \n",
    "    batch_count = int(self.dataset.train_count / batch_size)\n",
    "    report_period = epoch_count / 10\n",
    "    \n",
    "    if random_fix: np.random.seed(1945)\n",
    "    \n",
    "    time1 = time2 = int(time.time())\n",
    "    \n",
    "    print(\"Model {} train report:\".format(self.name))\n",
    "    \n",
    "    dset = self.dataset\n",
    "\n",
    "    for epoch in range(epoch_count):\n",
    "        costs = []\n",
    "        accs = []\n",
    "        for n in range(batch_count):\n",
    "            train_X, train_Y = dset.get_train_data(batch_size, n)\n",
    "            cost, acc = self.train_step(train_X, train_Y)\n",
    "            costs.append(cost)\n",
    "            accs.append(acc)\n",
    "            \n",
    "        if (epoch+1) % report_period == 0:\n",
    "            validate_X, validate_Y = dset.get_test_data(True, 30)\n",
    "            acc = self.get_accuracy(validate_X, validate_Y)\n",
    "            time3 = int(time.time())\n",
    "            print(\"    Epoch {}: cost={:5.3f}, \\\n",
    "accuracy={:5.3f}/{:5.3f} ({}/{} secs)\". \\\n",
    "                  format(epoch+1, np.mean(costs), np.mean(accs), \\\n",
    "                         acc, time3-time2, time3-time1))\n",
    "            time2 = time3\n",
    "\n",
    "    print(\"\")\n",
    "    \n",
    "    path = \"params/{}.ntpt\".format(self.name)\n",
    "    self.save_parameters(path)\n",
    "\n",
    "NoTFMLPModel.train = notf_model_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notf_model_test(self):\n",
    "    test_X, test_Y = self.dataset.get_test_data()\n",
    "\n",
    "    path = \"params/{}.ntpt\".format(self.name)\n",
    "    self.restore_parameters(path)\n",
    "    \n",
    "    time1 = int(time.time())\n",
    "    acc = self.get_accuracy(test_X, test_Y)\n",
    "    time2 = int(time.time())\n",
    "    \n",
    "    print(\"Model {} test report: accuracy = {:5.3f}, ({} secs)\". \\\n",
    "          format(self.name, acc, time2-time1))\n",
    "    print(\"\")\n",
    "    \n",
    "NoTFMLPModel.test = notf_model_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notf_model_demonstrate(self, num=10, batch=False):\n",
    "    demo_X, demo_Y = self.dataset.get_test_data(False, num)\n",
    "\n",
    "    path = \"params/{}.ntpt\".format(self.name)\n",
    "    self.restore_parameters(path)\n",
    "    \n",
    "    print(\"Model {} Demonstration\".format(self.name))\n",
    "    est, ans = self.get_estimate_answer(demo_X, demo_Y)\n",
    "    if batch:\n",
    "        self.dataset.demonstrate(demo_X, est, ans)\n",
    "    else:\n",
    "        for n in range(len(demo_X)):\n",
    "            self.dataset.demonstrate(demo_X[n], est[n], ans[n])\n",
    "        print(\"\")\n",
    "    \n",
    "NoTFMLPModel.demonstrate = notf_model_demonstrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(self, x, y):\n",
    "    output = self.proc_forward(x)\n",
    "\n",
    "    loss_grad = 1.0\n",
    "    \n",
    "    if self.dataset.mode == \"regression\":\n",
    "        diff = output - y\n",
    "        power = np.power(diff, 2)\n",
    "        loss = np.mean(power)\n",
    "        power_derv = np.ones_like(y) / np.prod(y.shape)\n",
    "        power_grad = power_derv * loss_grad\n",
    "        diff_derv = 2 * diff\n",
    "        diff_grad = diff_derv * power_grad\n",
    "        output_derv = 1\n",
    "        output_grad = output_derv * diff_grad\n",
    "    elif self.dataset.mode == \"binary\":\n",
    "        entropy = sigmoid_cross_entropy(y, output)\n",
    "        loss = np.mean(entropy)\n",
    "        ent_derv = np.ones_like(entropy) / np.prod(entropy.shape)\n",
    "        ent_grad = ent_derv * loss_grad\n",
    "        output_derv = sigmoid_cross_entropy_derv(y, output)\n",
    "        output_grad = output_derv * ent_grad\n",
    "    elif self.dataset.mode == \"select\":\n",
    "        probs = softmax(output)\n",
    "        entropy = softmax_cross_entropy(y, probs)\n",
    "        loss = np.mean(entropy)\n",
    "        ent_grad = loss_grad / np.prod(entropy.shape)\n",
    "        probs_derv = softmax_cross_entropy_derv(y, probs)\n",
    "        probs_grad = probs_derv * ent_grad\n",
    "        output_derv = softmax_derv(output, probs)\n",
    "        output_grad = [np.matmul(output_derv[n], probs_grad[n]) \\\n",
    "                       for n in range(output.shape[0])]\n",
    "    \n",
    "    self.proc_backward(x, output_grad)\n",
    "    \n",
    "    return loss, self.eval_accuracy(output, y)\n",
    "\n",
    "NoTFMLPModel.train_step = train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters(self, hidden_dims):\n",
    "    if random_fix: np.random.seed(9876)\n",
    "\n",
    "    input_dim = self.dataset.input_dim\n",
    "    output_dim = self.dataset.output_dim\n",
    "\n",
    "    self.weights, self.biases = [], []\n",
    "    \n",
    "    prev_dim = input_dim\n",
    "\n",
    "    for n in range(len(hidden_dims)):\n",
    "        next_dim = hidden_dims[n]\n",
    "        w = init_rand_normal(prev_dim, next_dim)\n",
    "        b = np.zeros([next_dim])\n",
    "        self.weights.append(w)\n",
    "        self.biases.append(b)\n",
    "        prev_dim = next_dim\n",
    "\n",
    "    w = notf_init_rand_normal(prev_dim, output_dim)\n",
    "    b = np.zeros([output_dim])\n",
    "    self.weights.append(w)\n",
    "    self.biases.append(b)\n",
    "    \n",
    "def notf_init_rand_normal(in_dim, out_dim, rand_std=0.0300):\n",
    "    init_64 = np.random.normal(0, rand_std, [in_dim, out_dim])\n",
    "    init = init_64.astype('float32')\n",
    "    return init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_forward(self, x):\n",
    "    self.hiddens = [x]\n",
    "\n",
    "    for n in range(len(self.hidden_dims)):\n",
    "        w, b = self.weights[n], self.biases[n]\n",
    "        hid = relu(np.matmul(self.hiddens[-1], w) + b)\n",
    "        self.hiddens.append(hid)\n",
    "    \n",
    "    w, b = self.weights[-1], self.biases[-1]\n",
    "    output = np.matmul(self.hiddens[-1], w) + b\n",
    "    \n",
    "    return output\n",
    "\n",
    "NoTFMLPModel.proc_forward = proc_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_backward(self, x, grad):\n",
    "    w_out_derv = self.hiddens[-1].transpose()\n",
    "    w_out_grad = np.matmul(w_out_derv, grad)\n",
    "    \n",
    "    b_out_grad = np.sum(grad, axis=0)\n",
    "    \n",
    "    hidden_derv = self.weights[-1].transpose()\n",
    "    hidden_grad = np.matmul(grad, hidden_derv)\n",
    "    \n",
    "    for n in range(len(self.hidden_dims))[::-1]:\n",
    "        hidden_affine_derv = relu_derv(self.hiddens[n+1])\n",
    "        hidden_affine_grad = hidden_affine_derv * hidden_grad\n",
    "    \n",
    "        w_hid_derv = self.hiddens[n].transpose()\n",
    "        w_hid_grad = np.matmul(w_hid_derv, hidden_affine_grad)\n",
    "    \n",
    "        b_hid_grad = np.sum(hidden_affine_grad, axis=0)\n",
    "        \n",
    "        hidden_derv = self.weights[n].transpose()\n",
    "        hidden_grad = np.matmul(hidden_affine_grad, hidden_derv)\n",
    "    \n",
    "        self.weights[n] = self.weights[n] - self.learning_rate * w_hid_grad\n",
    "        self.biases[n] = self.biases[n] - self.learning_rate * b_hid_grad\n",
    "    \n",
    "    self.weights[-1] = self.weights[-1] - self.learning_rate * w_out_grad\n",
    "    self.biases[-1] = self.biases[-1] - self.learning_rate * b_out_grad\n",
    "    \n",
    "NoTFMLPModel.proc_backward = proc_backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(self, x, y):\n",
    "    output = self.proc_forward(x)\n",
    "    return self.eval_accuracy(output, y)\n",
    "\n",
    "def eval_accuracy(self, output, y):\n",
    "    if self.dataset.mode == \"regression\":\n",
    "        diff = output - y\n",
    "        answer = y[:,0]\n",
    "        error = np.mean(np.abs(diff) / answer)\n",
    "        accuracy = 1 - error\n",
    "        probs = 0\n",
    "    elif self.dataset.mode == \"binary\":\n",
    "        #probs = sigmoid(output)\n",
    "        #estimate = np.greater(probs, 0.5)\n",
    "        estimate = np.greater(output, 0)\n",
    "        answer = np.equal(y, 1.0)\n",
    "        correct_bool = np.equal(estimate, answer)\n",
    "        #correct = np.cast(correct_bool, \"float32\")\n",
    "        accuracy = np.mean(correct_bool)\n",
    "    elif self.dataset.mode == \"select\":\n",
    "        #probs = softmax(output)\n",
    "        #estimate = np.argmax(probs, 1)\n",
    "        estimate = np.argmax(output, 1)\n",
    "        answer = np.argmax(y, 1)\n",
    "        correct_bool = np.equal(estimate, answer)\n",
    "        #correct = np.cast(correct_bool, \"float32\")\n",
    "        accuracy = np.mean(correct_bool)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "NoTFMLPModel.eval_accuracy = eval_accuracy\n",
    "NoTFMLPModel.get_accuracy = get_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimate_answer(self, x, y):\n",
    "    output = self.proc_forward(x)\n",
    "\n",
    "    if self.dataset.mode == \"regression\":\n",
    "        estimate = output[:,0]\n",
    "        answer = y[:,0]\n",
    "    elif self.dataset.mode == \"binary\":\n",
    "        #probs = sigmoid(output)\n",
    "        #estimate = np.greater(probs, 0.5)\n",
    "        estimate = np.greater(output, 0)\n",
    "        answer = np.equal(y, 1.0)\n",
    "    elif self.dataset.mode == \"select\":\n",
    "        #probs = softmax(output)\n",
    "        #estimate = np.argmax(probs, 1)\n",
    "        estimate = np.argmax(output, 1)\n",
    "        answer = np.argmax(y, 1)\n",
    "    \n",
    "    return estimate, answer\n",
    "\n",
    "NoTFMLPModel.get_estimate_answer = get_estimate_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_parameters(self, path):\n",
    "    np.savez(path, self.weights, self.biases)\n",
    "\n",
    "def restore_parameters(self, path):\n",
    "    fc = np.load(path)\n",
    "    self.weights, self.biases = fc['arr_0'], fc['arr_1']\n",
    "\n",
    "NoTFMLPModel.save_parameters = save_parameters\n",
    "NoTFMLPModel.restore_parameters = restore_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SudokuDataset(Dataset):\n",
    "    pass\n",
    "\n",
    "def unpack_map(line):\n",
    "    quiz = np.zeros([81], dtype=np.int8)\n",
    "    solution = np.zeros([81], dtype=np.int8)\n",
    "    for n in range(81):\n",
    "        quiz[n] = int(line[n])\n",
    "        solution[n] = int(line[n+82])\n",
    "    return quiz, solution\n",
    "\n",
    "def fill_hints(quiz, solution, hint_cnt):\n",
    "    for n in range(hint_cnt):\n",
    "        for k in range(1000):\n",
    "            pos = np.random.randint(81)\n",
    "            if quiz[pos] == 0:\n",
    "                quiz[pos] = solution[pos]\n",
    "                break\n",
    "    return quiz\n",
    "    \n",
    "def sudoku_init(self, train_ratio=0.80, valid_ratio=0.05):\n",
    "    Dataset.__init__(self, \"sudoku\")\n",
    "    \n",
    "    quizzes, solutions = [], []\n",
    "\n",
    "    max_count = 500000\n",
    "    \n",
    "    for line in open(\"./data/sudoku.csv\"):\n",
    "        if line[0] == 'q': continue\n",
    "        quiz, solution = unpack_map(line)\n",
    "        \n",
    "        #quiz = solution\n",
    "        #for n in range(5):\n",
    "        #    pos = np.random.randint(81)\n",
    "        #    quiz[pos] = 0\n",
    "        #quizzes.append(solution)\n",
    "        #solutions.append(solution)\n",
    "        \n",
    "        open_cnt = int((81-np.count_nonzero(quiz)) / 10)\n",
    "        for n in range(5):\n",
    "            quizzes.append(quiz)\n",
    "            solutions.append(solution)\n",
    "            quiz = fill_hints(quiz, solution, open_cnt)\n",
    "        if len(quizzes) >= max_count: break\n",
    "\n",
    "    ones = np.ones([81]).astype(int)\n",
    "    solution_idxes = solutions - ones\n",
    "    xs = np.asarray(quizzes)\n",
    "    ys = np.eye(9)[solution_idxes].reshape(-1, 81*9)\n",
    "\n",
    "    self.input_dim = 81\n",
    "    self.output_dim = 81*9\n",
    "\n",
    "    data_count = len(xs)\n",
    "    train_count = int(data_count * train_ratio)\n",
    "    valid_count = int(data_count * valid_ratio)\n",
    "    print('data_count', data_count)\n",
    "    print('train_count', train_count)\n",
    "    print('valid_count', valid_count)\n",
    "\n",
    "    test_start_idx = train_count + valid_count\n",
    "\n",
    "    indices = np.arange(data_count)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    self.train_xs = xs[indices[0:train_count]]\n",
    "    self.train_ys = ys[indices[0:train_count]]\n",
    "    self.validate_xs = xs[indices[train_count:test_start_idx]]\n",
    "    self.validate_ys = ys[indices[train_count:test_start_idx]]\n",
    "    self.test_xs = xs[indices[test_start_idx:]]\n",
    "    self.test_ys = ys[indices[test_start_idx:]]\n",
    "\n",
    "    self.train_count = train_count\n",
    "    \n",
    "def sudoku_demonstrate(self, x, y, estimate, answer, probs):\n",
    "    print(\"sudoku demonstrate dummy\")\n",
    "    \n",
    "SudokuDataset.__init__ = sudoku_init\n",
    "SudokuDataset.demonstrate = sudoku_demonstrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_count 500000\n",
      "train_count 400000\n",
      "valid_count 25000\n"
     ]
    }
   ],
   "source": [
    "sd = SudokuDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SudokuCnnModel(MultiLayerPerceptronModel):\n",
    "    pass\n",
    "\n",
    "def sudoku_init(self, name, dataset, hidden_dims, learning_rate=0.001):\n",
    "    MultiLayerPerceptronModel.__init__(self, name, dataset, hidden_dims, learning_rate)\n",
    "\n",
    "def sudoku_build_loss_accuracy(self):\n",
    "    self.temp_labels = tf.reshape(self.y, [-1, 81, 9])\n",
    "    self.temp_logits = tf.reshape(self.output, [-1, 81, 9])\n",
    "    self.probs = tf.nn.softmax(self.temp_logits)\n",
    "    #self.temp_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=self.temp_labels, logits=self.probs)\n",
    "    self.temp_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=self.temp_labels, logits=self.temp_logits)\n",
    "    self.loss = tf.reduce_mean(self.temp_cross_entropy)\n",
    "    self.temp_test = tf.argmax(self.temp_logits, 2)\n",
    "    self.estimate = tf.argmax(self.temp_logits, 2)\n",
    "    self.answer = tf.argmax(self.temp_labels, 2)\n",
    "    self.temp_correct = tf.cast(tf.equal(self.estimate, self.answer), \"float\")\n",
    "    self.accuracy = tf.reduce_mean(self.temp_correct)\n",
    "    self.max_probs = tf.reduce_max(self.probs, 2)\n",
    "    self.masked_max_probs = self.max_probs * tf.cast(1 - tf.sign(self.x), \"float\")\n",
    "    self.max_pos = tf.argmax(self.masked_max_probs, 1)\n",
    "    self.max_est = tf.argmax(self.probs, 2)\n",
    "    self.index_mask = tf.one_hot(self.max_pos, 81)\n",
    "    self.est_val = tf.reduce_sum(tf.cast(self.max_est, \"float\") * self.index_mask,1)\n",
    "    self.ans_val = tf.reduce_sum(tf.cast(self.answer, \"float\") * self.index_mask,1)\n",
    "    self.max_correct = tf.cast(tf.equal(self.est_val, self.ans_val), \"float\")\n",
    "    self.max_acc = tf.reduce_mean(self.max_correct)\n",
    "\n",
    "    #foo = tf.constant([[1,2,3], [4,5,6]])\n",
    "    #indexes = tf.constant([1,2]) #[1, 2])\n",
    "    #self.est_val = self.max_est[:, indexes]\n",
    "    #self.est_val = foo[:, indexes]\n",
    "    #self.est_val = self.max_est # self.max_est[:, self.max_pos]\n",
    "    \n",
    "    #foo = tf.constant([[1,2,3], [4,5,6]])\n",
    "    #foo[:, 1] # [2, 5]\n",
    "    #indexes = tf.constant([1, 2])\n",
    "    #foo[:, indexes] # [2, 6]\n",
    "\n",
    "\n",
    "    #self.est_val = self.max_est[self.max_pos]\n",
    "    #self.onehot = tf.cast(tf.one_hot(self.max_pos, 81), \"float\")\n",
    "    #self.max_est_val = tf.matmul(self.max_est, self.onehot) #tf.gather(self.max_est, self.max_pos)\n",
    "    #self.max_probs = tf.argmax(self.output, 2)\n",
    "    #self.est_masked = tf.multiply(self.est_max, tf.cast(1 - tf.sign(self.x), \"int64\"))\n",
    "    #self.est_max2 = tf.argmax(self.est_masked, 1)\n",
    "    #self.est_pos = tf.cast(tf.divide(self.est_max2, 10), \"int32\")\n",
    "    #self.est_val = self.est_masked % 10\n",
    "    #self.ans_val = self.est_val #tf.gather(self.answer, self.est_pos)\n",
    "    #self.max_correct = tf.cast(tf.equal(self.est_val, self.ans_val), \"float\")\n",
    "    #self.max_acc = tf.reduce_mean(self.max_correct)\n",
    "\n",
    "def sudoku_train(self, epoch_count=2, batch_size=10):\n",
    "    if batch_size == 0:\n",
    "        batch_size = self.dataset.train_count\n",
    "        \n",
    "    batch_count = int(self.dataset.train_count / batch_size)\n",
    "    report_period = epoch_count / 10\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    if random_fix: np.random.seed()\n",
    "    \n",
    "    time1 = time2 = int(time.time())\n",
    "    \n",
    "    print(\"Model {} train report:\".format(self.name))\n",
    "    \n",
    "    for epoch in range(epoch_count):\n",
    "        costs = []\n",
    "        accs = []\n",
    "        max_accs = []\n",
    "        for n in range(batch_count):\n",
    "            train_X, train_Y = self.dataset.get_train_data(batch_size, n)\n",
    "            _, labels, output, logits, entropy, cost, probs, estimate, answer, correct, \\\n",
    "                max_probs, masked_max_probs, max_pos, max_est, index_mask, est_val, ans_val, max_correct, max_acc, \\\n",
    "                acc = \\\n",
    "                sess.run([self.train_op, self.temp_labels, self.output, self.temp_logits, \n",
    "                          self.temp_cross_entropy, self.loss, self.probs, self.estimate, \n",
    "                          self.answer, self.temp_correct, \n",
    "                          self.max_probs, self.masked_max_probs, self.max_pos, self.max_est, self.index_mask, self.est_val, self.ans_val,\n",
    "                          self.max_correct, self.max_acc, \n",
    "                          self.accuracy], \\\n",
    "                                    feed_dict={self.x:train_X, self.y:train_Y})\n",
    "            #_, cost, acc = sess.run([self.train_op, self.loss, self.accuracy], \\\n",
    "            #                        feed_dict={self.x:train_X, self.y:train_Y})\n",
    "            costs.append(cost)\n",
    "            accs.append(acc)\n",
    "            max_accs.append(max_acc)\n",
    "        \n",
    "        if (epoch+1) % report_period == 0:\n",
    "            validate_X, validate_Y = self.dataset.get_test_data(True, 30)\n",
    "            acc = sess.run(self.accuracy, feed_dict={self.x:validate_X, self.y:validate_Y})\n",
    "            time3 = int(time.time())\n",
    "            print(\"    Epoch {}: cost={:5.3f}, accuracy={:5.3f}, {:5.3f}/{:5.3f}, {:5.3f} ({}/{} secs)\". \\\n",
    "                  format(epoch+1, np.mean(costs), acc, np.mean(accs), max_acc, np.mean(max_accs), time3-time2, time3-time1))\n",
    "            time2 = time3\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"probs\", np.shape(probs))\n",
    "    print(probs[0])\n",
    "    print(\"max_probs\", np.shape(max_probs))\n",
    "    print(max_probs[0])\n",
    "    print(\"train_X\", np.shape(train_X))\n",
    "    print(train_X[0])\n",
    "    print(\"masked_max_probs\", np.shape(masked_max_probs))\n",
    "    print(masked_max_probs[0])\n",
    "    print(\"max_pos\", max_pos)\n",
    "    print(\"max_est\", max_est)\n",
    "    print(\"index_mask\", index_mask)\n",
    "    print(\"answer\", answer)\n",
    "    \"\"\"\n",
    "    print(\"est_val\", est_val)\n",
    "    print(\"ans_val\", ans_val)\n",
    "    print(\"max_acc\", max_acc)\n",
    "    \n",
    "    #print(\"train_X\", np.shape(train_X))\n",
    "    #print(train_X[0])\n",
    "    #print(\"train_Y\", np.shape(train_Y))\n",
    "    #print(train_Y[0])\n",
    "    #print(\"labels\", np.shape(labels))\n",
    "    #print(labels[0])\n",
    "    #print(\"output\", np.shape(output))\n",
    "    #print(output[0])\n",
    "    #print(\"logits\", np.shape(logits))\n",
    "    #print(logits[0])\n",
    "    #print(\"probs\", np.shape(probs))\n",
    "    #print(np.max(probs[0], axis=1))\n",
    "    #print(\"est_masked\", np.shape(est_masked))\n",
    "    #print(est_masked[0])\n",
    "    #print(\"est_max\", np.shape(est_max))\n",
    "    #print(est_max)\n",
    "    #print(\"est_pos\", np.shape(est_pos))\n",
    "    #print(est_pos)\n",
    "    #print(\"est_val\", np.shape(est_val))\n",
    "    #print(est_val)\n",
    "    #print(\"ans_val\", np.shape(ans_val))\n",
    "    #print(ans_val)\n",
    "    #print(\"max_correct\", np.shape(max_correct))\n",
    "    #print(max_correct)\n",
    "    #print(\"max_acc\", max_acc)\n",
    "    \"\"\"\n",
    "    print(\"train_X\", np.shape(train_X))\n",
    "    print(\"x\", np.shape(x))\n",
    "    print(x[0])\n",
    "    print(\"y\", np.shape(y))\n",
    "    print(y[0])\n",
    "    print(\"labels\", np.shape(labels))\n",
    "    print(labels[0])\n",
    "    print(\"output\", np.shape(output))\n",
    "    print(output[0])\n",
    "    print(\"logits\", np.shape(logits))\n",
    "    print(logits[0])\n",
    "    print(\"probs\", np.shape(probs))\n",
    "    print(probs[0])\n",
    "    print(\"entropy\", np.shape(entropy))\n",
    "    print(entropy[0])\n",
    "    print(\"temp_test\", np.shape(temp_test))\n",
    "    print(temp_test[0])\n",
    "    print(\"estimate\", np.shape(estimate))\n",
    "    print(estimate[0])\n",
    "    print(\"answer\", np.shape(answer))\n",
    "    print(answer[0])\n",
    "    print(\"correct\", np.shape(correct))\n",
    "    print(correct[0])\n",
    "    print(\"acc\", np.shape(acc))\n",
    "    print(acc)\n",
    "\n",
    "    print(\"\")\n",
    "    \"\"\"\n",
    "    \n",
    "    path = \"params/{}.ckpt\".format(self.name)\n",
    "    self.saver.save(sess, path)\n",
    "    sess.close()\n",
    "\n",
    "SudokuCnnModel.__init__ = sudoku_init\n",
    "SudokuCnnModel.train = sudoku_train\n",
    "SudokuCnnModel.build_loss_accuracy = sudoku_build_loss_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sudoku_build_placeholders(self):\n",
    "    MultiLayerPerceptronModel.build_placeholders(self)\n",
    "    #self.cx = tf.placeholder(\"float\", [None, 27,9,1])\n",
    "    \n",
    "    \"\"\"\n",
    "    row, col, box = np.zeros([4,7]), np.zeros([4,7]), np.zeros([4,7])\n",
    "    \n",
    "    for m in range(4):\n",
    "        for n in range(7):\n",
    "            nn = n % 4\n",
    "            row[m][n] = m * 4 + nn\n",
    "            col[m][n] = nn * 4 + m\n",
    "            box[m][n] = (m//2*4+m%2) * 2 + (nn//2*4+nn%2)\n",
    "    \n",
    "    #print('box', box)\n",
    "    f2 = np.reshape([row, col, box], [-1]).astype(int)\n",
    "                \n",
    "    self.f2 = tf.constant(f2)\n",
    "    \"\"\"\n",
    "    \n",
    "    row, col, box = np.zeros([9,17]), np.zeros([9,17]), np.zeros([9,17])\n",
    "    \n",
    "    for m in range(9):\n",
    "        for n in range(17):\n",
    "            nn = n % 9\n",
    "            row[m][n] = m * 9 + nn\n",
    "            col[m][n] = nn * 9 + m\n",
    "            box[m][n] = (m//3*9+m%3) * 3 + (nn//3*9+nn%3)\n",
    "    \n",
    "    #print('box', box)\n",
    "    f3 = np.reshape([row, col, box], [-1]).astype(int)\n",
    "                \n",
    "    self.f3 = tf.constant(f3)\n",
    "\n",
    "    rev_map = np.zeros([9,9,3])\n",
    "    \n",
    "    for r in range(9):\n",
    "        for c in range(9):\n",
    "            rev_map[r,c,0] = r * 9 + c\n",
    "            rev_map[r,c,1] = 81 + c * 9 + r\n",
    "            rev_map[r,c,2] = 162 + ((r//3)*3+c//3) * 9 + (r%3)*3+c%3\n",
    "\n",
    "    #print(\"rev_map\", rev_map);\n",
    "                    \n",
    "    frev3 = np.reshape(rev_map, [-1]).astype(int)\n",
    "                \n",
    "    self.ff3 = tf.constant(frev3)\n",
    "\n",
    "    \"\"\"\n",
    "    #cube2, cube3 = np.zeros([2,4,4]), np.zeros([2,9,9])\n",
    "    cube3 = np.zeros([2,4,4])\n",
    "\n",
    "    for k in range(2):\n",
    "        for m in range(4):\n",
    "            for n in range(4):\n",
    "                cube2[k][m][n] = (k+1)*100+(m+1)*10+(n+1)\n",
    "    \n",
    "    for k in range(2):\n",
    "        for m in range(9):\n",
    "            for n in range(9):\n",
    "                cube3[k][m][n] = (k+1)*100+(m+1)*10+(n+1)\n",
    "                \n",
    "    self.x2 = tf.constant(np.reshape(cube2, [2,16]).astype(np.float32))\n",
    "    self.x3 = tf.constant(np.reshape(cube3, [2,81]).astype(np.float32))\n",
    "\n",
    "    #self.f2 = tf.constant([[0,4,8,12,1,5,9,13,2,6,10,14,3,7,11,15]])\n",
    "    \n",
    "    self.g2 = tf.reshape(tf.gather(self.x2, self.f2, axis=1), [-1, 12, 4+3, 1])\n",
    "    \"\"\"\n",
    "\n",
    "def sudoku_build_parameters(self, hidden_dim):\n",
    "    MultiLayerPerceptronModel.build_parameters(self, [hidden_dim])\n",
    "    #self.kernel_xw = tf.Variable(tf.ones([1, 9, 1, hidden_dim], tf.float32)) #tf.random_normal([1,9,1,hidden_dim], stddev=0.03))\n",
    "    #self.kernel_xb = tf.Variable(tf.zeros([hidden_dim]))\n",
    "    #w = np.zeros([1, 9, 1, hidden_dim], dtype=\"float32\")\n",
    "    #w[0,0,0,0] = 1\n",
    "    #w[0,:,0,1] = 1\n",
    "    self.kernel_xw = tf.Variable(tf.random_normal([1,9,1,hidden_dim], stddev=0.03))\n",
    "    self.kernel_xb = tf.Variable(tf.zeros([hidden_dim]))\n",
    "\n",
    "    #w2 = np.zeros([1, 1, 3*hidden_dim, 9], dtype=\"float32\")\n",
    "    #w2[0,0,0,0] = 1\n",
    "    #w2[0,:,0,1] = 1\n",
    "    self.kernel_xw2 = tf.Variable(tf.random_normal([1, 1, 3*hidden_dim, 9], stddev=0.03))\n",
    "    self.kernel_xb2 = tf.Variable(tf.zeros([9]))\n",
    "\n",
    "def sudoku_build_neuralnet(self, hidden_dim):\n",
    "    MultiLayerPerceptronModel.build_neuralnet(self, [hidden_dim])\n",
    "    \n",
    "    self.g3 = tf.reshape(tf.gather(self.x, self.f3, axis=1), [-1, 27, 9+8, 1])\n",
    "\n",
    "    #conv_linear = tf.nn.conv2d(self.g3, self.kernel_xw, strides=[1, 1, 9, 1], padding='SAME', data_format='NHWC')\n",
    "    conv_linear = tf.nn.conv2d(self.g3, self.kernel_xw, strides=[1, 1, 1, 1], padding='VALID', data_format='NHWC')\n",
    "    conv_with_b = tf.nn.bias_add(conv_linear, self.kernel_xb, data_format='NHWC')\n",
    "    conv_out = tf.reshape(tf.nn.relu(conv_with_b), [-1,243,hidden_dim])\n",
    "    self.conv2_in = tf.reshape(tf.gather(conv_out, self.ff3, axis=1), [-1, 81, 1, 3*hidden_dim])\n",
    "    \n",
    "    conv_linear2 = tf.nn.conv2d(self.conv2_in, self.kernel_xw2, strides=[1, 1, 1, 1], padding='VALID', data_format='NHWC')\n",
    "    conv_with_b2 = tf.nn.bias_add(conv_linear2, self.kernel_xb2, data_format='NHWC')\n",
    "    conv_out2 = tf.nn.relu(conv_with_b2)\n",
    "\n",
    "    self.conv_out = tf.reshape(tf.nn.relu(conv_out2), [-1, 81, 9])\n",
    "    self.output = self.conv_out\n",
    "\n",
    "SudokuCnnModel.build_placeholders = sudoku_build_placeholders\n",
    "SudokuCnnModel.build_parameters = sudoku_build_parameters\n",
    "SudokuCnnModel.build_neuralnet = sudoku_build_neuralnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sudoku_dump(self, batch_size=100):\n",
    "    if batch_size == 0:\n",
    "        batch_size = self.dataset.train_count\n",
    "        \n",
    "    batch_count = int(self.dataset.train_count / batch_size)\n",
    "    #report_period = epoch_count / 10\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    n = 0\n",
    "    \n",
    "    #x2, f2, g2, x3, f3, g3 = sess.run([self.x2, self.f2, self.g2, self.x3, self.f3, self.g3])\n",
    "\n",
    "    #print('x2', x2)\n",
    "    #print('f2', f2)\n",
    "    #print('g2', g2)\n",
    "    #print('x3', x3)\n",
    "    #print('f3', f3)\n",
    "    #print('g3', g3)\n",
    "    #print('x3', x3)\n",
    "    \n",
    "    train_X, train_Y = self.dataset.get_train_data(batch_size, n)\n",
    "    #reformed_X = np.reshape(train_X[:,self.x_reformer], [-1,27,9,1])\n",
    "    \n",
    "    #print('reformed_X', reformed_X)\n",
    "\n",
    "    #kernel_xw, kernel_xb = sess.run([self.kernel_xw, self.kernel_xb])\n",
    "\n",
    "    #print('kernel_xw', kernel_xw)\n",
    "    #print('kernel_xb', kernel_xb)\n",
    "    \n",
    "    loss, est_val, ans_val, max_acc, acc, conv2_in, conv_out = \\\n",
    "        sess.run([self.loss, self.est_val, self.ans_val, self.max_acc, self.accuracy, self.conv2_in, self.conv_out], \\\n",
    "                 feed_dict={self.x:train_X, self.y:train_Y})\n",
    "\n",
    "    #print('conv_out.shape', np.shape(conv_out))\n",
    "    #print('conv_out', np.shape(conv_out), conv_out)\n",
    "    #print('conv2_in', np.shape(conv2_in), conv2_in)\n",
    "    print(\"est_val\", est_val)\n",
    "    print(\"ans_val\", ans_val)\n",
    "    print(\"max_acc\", max_acc)\n",
    "    \n",
    "    print('loss', loss)\n",
    "\n",
    "    print('done')\n",
    "    \n",
    "    sess.close()\n",
    "\n",
    "SudokuCnnModel.dump = sudoku_dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sudoku-cnn-2 train report:\n"
     ]
    }
   ],
   "source": [
    "sm2 = SudokuCnnModel(\"sudoku-cnn-2\", sd, 128, learning_rate=0.001)\n",
    "sm2.train(epoch_count=1000, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
