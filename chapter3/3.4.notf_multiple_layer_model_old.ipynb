{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_fix = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, mode):\n",
    "        self.mode = mode\n",
    "        self.input_dim = self.output_dim = self.train_count = 0\n",
    "        self.train_xs = self.test_xs = self.validate_xs = []\n",
    "        self.train_ys = self.test_ys = self.validate_ys = []\n",
    "        self.target_names = []\n",
    "    \n",
    "    def get_train_data(self, batch_size, nth):\n",
    "        if nth == 0:\n",
    "            self.indices = np.arange(self.train_count)\n",
    "            np.random.shuffle(self.indices)\n",
    "        \n",
    "        from_idx = nth * batch_size\n",
    "        to_idx = (nth + 1) * batch_size\n",
    "    \n",
    "        train_X = self.train_xs[self.indices[from_idx:to_idx]]\n",
    "        train_Y = self.train_ys[self.indices[from_idx:to_idx]]\n",
    "        \n",
    "        return train_X, train_Y    \n",
    "    \n",
    "    def get_test_data(self, validate=False, count=0):\n",
    "        if validate:\n",
    "            xs, ys = self.validate_xs, self.validate_ys\n",
    "        else:\n",
    "            xs, ys = self.test_xs, self.test_ys\n",
    "        \n",
    "        if count == 0:\n",
    "            return xs, ys\n",
    "        \n",
    "        if count > len(xs): count = len(xs)\n",
    "        \n",
    "        indices = np.arange(len(xs))\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        return xs[indices[0:count]], ys[indices[0:count]]\n",
    "    \n",
    "    def get_target_name(self, idxs):\n",
    "        return self.target_names[idxs]\n",
    "    \n",
    "    def demonstrate(self, x, estimate, answer):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, name, dataset):\n",
    "        self.name = name\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def train(self, epoch=10, batch_size=10):\n",
    "        pass\n",
    "    \n",
    "    def test(self):\n",
    "        pass\n",
    "    \n",
    "    def demonstrate(self, num):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "class IrisDataset(Dataset):\n",
    "    pass\n",
    "\n",
    "def iris_init(self, mode, train_ratio=0.8):\n",
    "    Dataset.__init__(self, mode)\n",
    "    \n",
    "    dataset = datasets.load_iris()\n",
    "    \n",
    "    xs, ys = iris_prepare(self, mode, dataset.data, dataset.target)\n",
    "    \n",
    "    data_count = len(dataset.data)\n",
    "    self.train_count = int(data_count * train_ratio)\n",
    "\n",
    "    indices = np.arange(data_count)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    self.train_xs = xs[indices[0:self.train_count]]\n",
    "    self.train_ys = ys[indices[0:self.train_count]]\n",
    "    self.test_xs = self.validate_xs = xs[indices[self.train_count:]]\n",
    "    self.test_ys = self.validate_ys = ys[indices[self.train_count:]]\n",
    "    \n",
    "    self.target_names = dataset.target_names\n",
    "\n",
    "def iris_prepare(self, mode, data, target):\n",
    "    if mode == \"regression\":\n",
    "        self.input_dim = 3\n",
    "        self.output_dim = 1\n",
    "        xs = data[:, 0:3]\n",
    "        ys = data[:, 3:4]\n",
    "    elif mode == \"binary\":\n",
    "        self.input_dim = 4\n",
    "        self.output_dim = 1\n",
    "        xs = data\n",
    "        ys = np.equal(target, 0).astype(\"float32\").reshape(-1,1)\n",
    "    elif mode == \"select\":\n",
    "        self.input_dim = 4\n",
    "        self.output_dim = 3\n",
    "        xs = data\n",
    "        ys = np.eye(3)[target]\n",
    "        \n",
    "    return xs, ys\n",
    "\n",
    "def iris_demonstrate(self, x, estimate, answer):\n",
    "    if self.mode == \"regression\":\n",
    "        print(\"({}, {}, {}) => 추정 {:3.1f}, 정답: {:3.1f}\".format(x[0], x[1], x[2], estimate, answer))\n",
    "    elif self.mode == \"binary\":\n",
    "        estr = \"is setosa\"\n",
    "        astr = \"(오답)\"\n",
    "        if not estimate: estr = \"is not setosa\"\n",
    "        if estimate == answer: astr = \"(정답)\"\n",
    "        print(\"({}, {}, {}, {}) => {} {}\".format(x[0], x[1], x[2], x[3], estr, astr))\n",
    "    elif self.mode == \"select\":\n",
    "        estr = self.target_names[estimate]\n",
    "        astr = \"({})\".format(self.target_names[answer])\n",
    "        if estimate == answer: astr = \"(정답)\"\n",
    "        print(\"({}, {}, {}, {}) => {} {}\".format(x[0], x[1], x[2], x[3], estr, astr))\n",
    "    \n",
    "IrisDataset.__init__ = iris_init\n",
    "IrisDataset.demonstrate = iris_demonstrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "id1 = IrisDataset(\"regression\")\n",
    "id2 = IrisDataset(\"binary\", 0.88)\n",
    "id3 = IrisDataset(\"select\", 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MnistDataset(Dataset):\n",
    "    pass\n",
    "\n",
    "def mnist_init(self):\n",
    "    Dataset.__init__(self, \"select\")\n",
    "    \n",
    "    dataset = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "        \n",
    "    self.train_xs = dataset.train.images\n",
    "    self.train_ys = dataset.train.labels\n",
    "        \n",
    "    self.validate_xs = dataset.validation.images\n",
    "    self.validate_ys = dataset.validation.labels\n",
    "        \n",
    "    self.test_xs = dataset.test.images\n",
    "    self.test_ys = dataset.test.labels\n",
    "\n",
    "    self.input_dim = 28 * 28\n",
    "    self.output_dim = 10\n",
    "    \n",
    "    self.train_count = len(dataset.train.images)\n",
    "    self.target_names = ['0','1','2','3','4','5','6','7','8','9']\n",
    "\n",
    "def mnist_demonstrate(self, xs, est, ans):\n",
    "    rows, cols = 4, 4\n",
    "    f, a = plt.subplots(rows, cols, figsize=(cols, rows))\n",
    "    for row in range(cols):\n",
    "        for col in range(cols):\n",
    "            i = row * cols + col\n",
    "            estr = self.target_names[est[i]]\n",
    "            astr = self.target_names[ans[i]]\n",
    "            if est[i] == ans[i]:\n",
    "                caption = \"{}\".format(estr)\n",
    "            else:\n",
    "                caption = \"{}=>{}\".format(astr, estr)\n",
    "            a[row][col].axvspan(0, 0, 0, 6.0)\n",
    "            a[row][col].imshow(np.reshape(xs[i], (28,28)))\n",
    "            a[row][col].text(0.5, -1.5, caption)\n",
    "            a[row][col].axis('off')\n",
    "    f.show()\n",
    "    plt.draw()\n",
    "    plt.show()\n",
    "        \n",
    "MnistDataset.__init__ = mnist_init\n",
    "MnistDataset.demonstrate = mnist_demonstrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "md = MnistDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNoTFModel(Model):\n",
    "    pass\n",
    "\n",
    "def mlp_init(self, name, dataset, hidden_dims, learning_rate=0.001):\n",
    "    Model.__init__(self, name, dataset)\n",
    "    init_parameters(self, hidden_dims)\n",
    "    self.learning_rate = learning_rate\n",
    "\n",
    "MLPNoTFModel.__init__ = mlp_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_rand_normal(in_dim, out_dim, rand_std=0.0300):\n",
    "    init_64 = np.random.normal(0, rand_std, [in_dim, out_dim])\n",
    "    init = init_64.astype('float32')\n",
    "    return init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef build_neuralnet(self, hidden_dims):\\n    hidden = self.x\\n    for n in range(len(hidden_dims)):\\n        affine = tf.matmul(hidden, self.w_hids[n]) + self.b_hids[n]\\n        hidden = tf.nn.relu(affine)\\n    self.output = tf.matmul(hidden, self.w_out) + self.b_out\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_parameters(self, hidden_dims):\n",
    "    if random_fix: np.random.seed(9876)\n",
    "\n",
    "    input_dim = self.dataset.input_dim\n",
    "    output_dim = self.dataset.output_dim\n",
    "\n",
    "    self.weights, self.biases = [], []\n",
    "    \n",
    "    prev_dim = input_dim\n",
    "\n",
    "    for n in range(len(hidden_dims)):\n",
    "        next_dim = hidden_dims[n]\n",
    "        w = init_rand_normal(prev_dim, next_dim)\n",
    "        b = np.zeros([next_dim])\n",
    "        self.weights.append(w)\n",
    "        self.biases.append(b)\n",
    "        prev_dim = next_dim\n",
    "\n",
    "    w = init_rand_normal(prev_dim, output_dim)\n",
    "    b = np.zeros([output_dim])\n",
    "    self.weights.append(w)\n",
    "    self.biases.append(b)\n",
    "    \n",
    "    self.hidden_dims = hidden_dims\n",
    "\n",
    "\"\"\"\n",
    "def build_neuralnet(self, hidden_dims):\n",
    "    hidden = self.x\n",
    "    for n in range(len(hidden_dims)):\n",
    "        affine = tf.matmul(hidden, self.w_hids[n]) + self.b_hids[n]\n",
    "        hidden = tf.nn.relu(affine)\n",
    "    self.output = tf.matmul(hidden, self.w_out) + self.b_out\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef build_loss_accuracy(self):\\n    if self.dataset.mode == \"regression\":\\n        self.estimate = self.output[:,0]\\n        self.answer = self.y[:,0]\\n        diff = self.estimate - self.answer\\n        self.loss = tf.reduce_mean(tf.pow(diff, 2))\\n        error = tf.reduce_mean(tf.abs(diff) / self.answer)\\n        self.accuracy = 1 - error\\n    elif self.dataset.mode == \"binary\":\\n        cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.y, logits=self.output)\\n        self.loss = tf.reduce_mean(cross_entropy)\\n        #probs = tf.nn.sigmoid(output)\\n        #estimate = tf.greater(probs, 0.5)\\n        self.estimate = tf.greater(self.output, 0)\\n        self.answer = tf.equal(self.y, 1.0)\\n        correct_bool = tf.equal(self.estimate, self.answer)\\n        correct = tf.cast(correct_bool, \"float\")\\n        self.accuracy = tf.reduce_mean(correct)\\n    elif self.dataset.mode == \"select\":\\n        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=self.y, logits=self.output)\\n        self.loss = tf.reduce_mean(cross_entropy)\\n        #probs = tf.nn.softmax(output)\\n        #estimate = tf.argmax(probs, 1)\\n        self.estimate = tf.argmax(self.output, 1)\\n        self.answer = tf.argmax(self.y, 1)\\n        correct_bool = tf.equal(self.estimate, self.answer)\\n        correct = tf.cast(correct_bool, \"float\")\\n        self.accuracy = tf.reduce_mean(correct)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def build_loss_accuracy(self):\n",
    "    if self.dataset.mode == \"regression\":\n",
    "        self.estimate = self.output[:,0]\n",
    "        self.answer = self.y[:,0]\n",
    "        diff = self.estimate - self.answer\n",
    "        self.loss = tf.reduce_mean(tf.pow(diff, 2))\n",
    "        error = tf.reduce_mean(tf.abs(diff) / self.answer)\n",
    "        self.accuracy = 1 - error\n",
    "    elif self.dataset.mode == \"binary\":\n",
    "        cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.y, logits=self.output)\n",
    "        self.loss = tf.reduce_mean(cross_entropy)\n",
    "        #probs = tf.nn.sigmoid(output)\n",
    "        #estimate = tf.greater(probs, 0.5)\n",
    "        self.estimate = tf.greater(self.output, 0)\n",
    "        self.answer = tf.equal(self.y, 1.0)\n",
    "        correct_bool = tf.equal(self.estimate, self.answer)\n",
    "        correct = tf.cast(correct_bool, \"float\")\n",
    "        self.accuracy = tf.reduce_mean(correct)\n",
    "    elif self.dataset.mode == \"select\":\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=self.y, logits=self.output)\n",
    "        self.loss = tf.reduce_mean(cross_entropy)\n",
    "        #probs = tf.nn.softmax(output)\n",
    "        #estimate = tf.argmax(probs, 1)\n",
    "        self.estimate = tf.argmax(self.output, 1)\n",
    "        self.answer = tf.argmax(self.y, 1)\n",
    "        correct_bool = tf.equal(self.estimate, self.answer)\n",
    "        correct = tf.cast(correct_bool, \"float\")\n",
    "        self.accuracy = tf.reduce_mean(correct)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef build_optimizer(self, learning_rate):\\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\\n    self.train_op = optimizer.minimize(self.loss)\\n\\ndef build_saver(self):\\n    var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\\n    self.saver = tf.train.Saver(var_list=var_list)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def build_optimizer(self, learning_rate):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    self.train_op = optimizer.minimize(self.loss)\n",
    "\n",
    "def build_saver(self):\n",
    "    var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
    "    self.saver = tf.train.Saver(var_list=var_list)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def model_train(self, epoch_count=10, batch_size=10):\n",
    "    if batch_size == 0:\n",
    "        batch_size = self.dataset.train_count\n",
    "        \n",
    "    batch_count = int(self.dataset.train_count / batch_size)\n",
    "    report_period = epoch_count / 10\n",
    "    \n",
    "    if random_fix: np.random.seed(1945)\n",
    "    \n",
    "    time1 = time2 = int(time.time())\n",
    "    \n",
    "    print(\"Model {} train report:\".format(self.name))\n",
    "    \n",
    "    dset = self.dataset\n",
    "\n",
    "    print(self.weights)\n",
    "    for epoch in range(epoch_count):\n",
    "        costs = []\n",
    "        accs = []\n",
    "        for n in range(batch_count):\n",
    "            train_X, train_Y = dset.get_train_data(batch_size, n)\n",
    "            cost, acc = self.train_step(train_X, train_Y)\n",
    "            print(\"step{}: cost:{}, acc:{}\".format(n, cost, acc))\n",
    "            costs.append(cost)\n",
    "            accs.append(acc)\n",
    "            \n",
    "        if (epoch+1) % report_period == 0:\n",
    "            validate_X, validate_Y = dset.get_test_data(True, 30)\n",
    "            acc = self.get_accuracy(validate_X, validate_Y)\n",
    "            time3 = int(time.time())\n",
    "            print(\"    Epoch {}: cost={:5.3f}, \\\n",
    "accuracy={:5.3f}/{:5.3f} ({}/{} secs)\". \\\n",
    "                  format(epoch+1, np.mean(costs), np.mean(accs), \\\n",
    "                         acc, time3-time2, time3-time1))\n",
    "            time2 = time3\n",
    "\n",
    "    print(\"\")\n",
    "    \n",
    "    path = \"params/{}.ntpt\".format(self.name)\n",
    "    self.save_parameters(path)\n",
    "\n",
    "Model.train = model_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(self):\n",
    "    test_X, test_Y = self.dataset.get_test_data()\n",
    "\n",
    "    path = \"params/{}.ntpt\".format(self.name)\n",
    "    self.restore_parameters(path)\n",
    "    \n",
    "    time1 = int(time.time())\n",
    "    acc = self.get_accuracy(test_X, test_Y)\n",
    "    time2 = int(time.time())\n",
    "    \n",
    "    print(\"Model {} test report: accuracy = {:5.3f}, ({} secs)\".format(self.name, acc, time2-time1))\n",
    "    print(\"\")\n",
    "    \n",
    "Model.test = model_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_demonstrate(self, num=10, batch=False):\n",
    "    demo_X, demo_Y = self.dataset.get_test_data(False, num)\n",
    "\n",
    "    path = \"params/{}.ntpt\".format(self.name)\n",
    "    self.restore_parameters(path)\n",
    "    \n",
    "    print(\"Model {} Demonstration\".format(self.name))\n",
    "    est, ans = self.get_estimate_answer(demo_X, demo_Y)\n",
    "    if batch:\n",
    "        self.dataset.demonstrate(demo_X, est, ans)\n",
    "    else:\n",
    "        for n in range(len(demo_X)):\n",
    "            self.dataset.demonstrate(demo_X[n], est[n], ans[n])\n",
    "        print(\"\")\n",
    "    \n",
    "Model.demonstrate = model_demonstrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(self, x, y):\n",
    "    output = self.proc_forward(x)\n",
    "\n",
    "    if self.dataset.mode == \"regression\":\n",
    "        diff = output - y\n",
    "        power = np.power(diff, 2)\n",
    "        loss = np.mean(power)\n",
    "        loss_grad = 1.0\n",
    "        power_derv = np.ones_like(y) / np.prod(y.shape)\n",
    "        power_grad = power_derv * loss_grad\n",
    "        diff_derv = 2 * diff\n",
    "        diff_grad = diff_derv * power_grad\n",
    "        output_derv = 1\n",
    "        output_grad = output_derv * diff_grad\n",
    "    elif self.dataset.mode == \"binary\":\n",
    "        entropy = sigmoid_cross_entropy(y, output)\n",
    "        loss = np.mean(entropy)\n",
    "        loss_grad = 1.0\n",
    "        ent_derv = np.ones_like(entropy) / np.prod(entropy.shape)\n",
    "        ent_grad = ent_derv * loss_grad\n",
    "        output_derv = sigmoid_cross_entropy_derv(y, output)\n",
    "        output_grad = output_derv * ent_grad\n",
    "    elif self.dataset.mode == \"select\":\n",
    "        probs = softmax(output)\n",
    "        entropy = softmax_cross_entropy(y, probs)\n",
    "        loss = np.mean(entropy)\n",
    "        loss_grad = 1.0\n",
    "        ent_grad = loss_grad / np.prod(entropy.shape)\n",
    "        probs_derv = softmax_cross_entropy_derv(y, probs)\n",
    "        probs_grad = probs_derv * ent_grad\n",
    "        output_derv = softmax_derv(output, probs)\n",
    "        output_grad = [np.matmul(output_derv[n], probs_grad[n]) \\\n",
    "                       for n in range(output.shape[0])]\n",
    "    \n",
    "    self.proc_backward(x, output_grad)\n",
    "    \n",
    "    return loss, self.eval_accuracy(output, y)\n",
    "\n",
    "MLPNoTFModel.train_step = train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_forward(self, x):\n",
    "    self.hiddens = [x]\n",
    "\n",
    "    for n in range(len(self.hidden_dims)):\n",
    "        hid = relu(np.matmul(self.hiddens[-1], self.weights[n]) + self.biases[n])\n",
    "        self.hiddens.append(hid)\n",
    "    \n",
    "    output = np.matmul(self.hiddens[-1], self.weights[-1]) + self.biases[-1]\n",
    "    \n",
    "    self.hiddens.append(output)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def proc_backward(self, x, grad):\n",
    "    w_out_derv = self.hiddens[-1].transpose()\n",
    "    w_out_grad = np.matmul(w_out_derv, grad)\n",
    "    \n",
    "    b_out_grad = np.sum(grad, axis=0)\n",
    "    \n",
    "    hidden_derv = self.weights[-1].transpose()\n",
    "    hidden_grad = np.matmul(grad, hidden_derv)\n",
    "    \n",
    "    for n in range(len(self.hidden_dims))[::-1]:\n",
    "        hidden_affine_derv = relu_derv(self.hiddens[n+1])\n",
    "        hidden_affine_grad = hidden_affine_derv * hidden_grad\n",
    "    \n",
    "        w_hid_derv = self.hiddens[n].transpose()\n",
    "        w_hid_grad = np.matmul(w_hid_derv, hidden_affine_grad)\n",
    "    \n",
    "        b_hid_grad = np.sum(hidden_affine_grad, axis=0)\n",
    "        \n",
    "        grad = hidden_affine_grad\n",
    "    \n",
    "        hidden_derv = self.weights[n].transpose()\n",
    "        hidden_grad = np.matmul(grad, hidden_derv)\n",
    "    \n",
    "        self.weights[n] = self.weights[n] - self.learning_rate * w_hid_grad\n",
    "        self.biases[n] = self.biases[n] - self.learning_rate * b_hid_grad\n",
    "    \n",
    "    self.weights[-1] = self.weights[-1] - self.learning_rate * w_out_grad\n",
    "    self.biases[-1] = self.biases[-1] - self.learning_rate * b_out_grad\n",
    "    \n",
    "MLPNoTFModel.proc_forward = proc_forward\n",
    "MLPNoTFModel.proc_backward = proc_backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(self, x, y):\n",
    "    output = self.proc_forward(x)\n",
    "    return self.eval_accuracy(output, y)\n",
    "\n",
    "def eval_accuracy(self, output, y):\n",
    "    if self.dataset.mode == \"regression\":\n",
    "        diff = output - y\n",
    "        answer = y[:,0]\n",
    "        error = np.mean(np.abs(diff) / answer)\n",
    "        accuracy = 1 - error\n",
    "    elif self.dataset.mode == \"binary\":\n",
    "        #probs = sigmoid(output)\n",
    "        #estimate = np.greater(probs, 0.5)\n",
    "        estimate = np.greater(output, 0)\n",
    "        answer = np.equal(y, 1.0)\n",
    "        correct_bool = np.equal(estimate, answer)\n",
    "        #correct = np.cast(correct_bool, \"float32\")\n",
    "        accuracy = np.mean(correct_bool)\n",
    "    elif self.dataset.mode == \"select\":\n",
    "        #probs = softmax(output)\n",
    "        #estimate = np.argmax(probs, 1)\n",
    "        estimate = np.argmax(output, 1)\n",
    "        answer = np.argmax(y, 1)\n",
    "        correct_bool = np.equal(estimate, answer)\n",
    "        #correct = np.cast(correct_bool, \"float32\")\n",
    "        accuracy = np.mean(correct_bool)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def get_estimate_answer(self, x, y):\n",
    "    output = self.proc_forward(x)\n",
    "\n",
    "    if self.dataset.mode == \"regression\":\n",
    "        estimate = output[:,0]\n",
    "        answer = y[:,0]\n",
    "    elif self.dataset.mode == \"binary\":\n",
    "        #probs = sigmoid(output)\n",
    "        #estimate = np.greater(probs, 0.5)\n",
    "        estimate = np.greater(output, 0)\n",
    "        answer = np.equal(y, 1.0)\n",
    "    elif self.dataset.mode == \"select\":\n",
    "        #probs = softmax(output)\n",
    "        #estimate = np.argmax(probs, 1)\n",
    "        estimate = np.argmax(output, 1)\n",
    "        answer = np.argmax(y, 1)\n",
    "    \n",
    "    return estimate, answer\n",
    "\n",
    "MLPNoTFModel.eval_accuracy = eval_accuracy\n",
    "MLPNoTFModel.get_accuracy = get_accuracy\n",
    "MLPNoTFModel.get_estimate_answer = get_estimate_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_parameters(self, path):\n",
    "    pass\n",
    "\n",
    "def restore_parameters(self, path):\n",
    "    pass\n",
    "\n",
    "MLPNoTFModel.save_parameters = save_parameters\n",
    "MLPNoTFModel.restore_parameters = restore_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):#3.2.1.8\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def relu_derv(y):\n",
    "    return np.sign(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x): #2.6.9\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derv(x, y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "def sigmoid_cross_entropy(z, x):\n",
    "    return np.maximum(x, 0) - x * z + np.log(1 + np.exp(-np.abs(x)))\n",
    "\n",
    "def sigmoid_cross_entropy_derv(z, x):\n",
    "    return -z + sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    max_elem = np.max(x, axis=1)\n",
    "    diff = (x.transpose() - max_elem).transpose()\n",
    "    exp = np.exp(diff)\n",
    "    sum_exp = np.sum(exp, axis=1)\n",
    "    probs = (exp.transpose() / sum_exp).transpose()\n",
    "    return probs\n",
    "\n",
    "def softmax_derv(x, y):\n",
    "    mb_size, nom_size = x.shape\n",
    "    derv = np.ndarray([mb_size, nom_size, nom_size])\n",
    "    for n in range(mb_size):\n",
    "        for i in range(nom_size):\n",
    "            for j in range(nom_size):\n",
    "                derv[n, i, j] = -y[n,i] * y[n,j]\n",
    "            derv[n, i, i] += y[n,i]\n",
    "    return derv\n",
    "\n",
    "def softmax_cross_entropy(p, q):\n",
    "    return -np.sum(p * np.log(q), axis=1)\n",
    "\n",
    "def softmax_cross_entropy_derv(p, q):\n",
    "    return -p / q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model notf-iris-regression-hid-1 train report:\n",
      "[array([[ 0.01173613, -0.03273037,  0.00643122,  0.00968027, -0.02775064,\n",
      "         0.00091096, -0.03340559, -0.00369707, -0.00038589,  0.01969427],\n",
      "       [-0.00980483, -0.00899604, -0.02689693, -0.03099372,  0.0066654 ,\n",
      "        -0.00755667,  0.02855561,  0.03363259,  0.01013013,  0.01377459],\n",
      "       [ 0.02621648, -0.00263455,  0.01591358,  0.02987679,  0.05952203,\n",
      "         0.01030967,  0.0228562 ,  0.00307394, -0.03339037,  0.01263231]], dtype=float32), array([[ 0.03830937],\n",
      "       [-0.04520908],\n",
      "       [ 0.0265373 ],\n",
      "       [ 0.03551965],\n",
      "       [ 0.02676535],\n",
      "       [-0.01836309],\n",
      "       [-0.02519594],\n",
      "       [ 0.02597552],\n",
      "       [ 0.05982467],\n",
      "       [-0.00786958]], dtype=float32)]\n",
      "step0: cost:1.9321341950563824, acc:-1.2129872262835595\n",
      "step1: cost:2.697365506041181, acc:-0.6337946625920798\n",
      "step2: cost:1.3922001540232831, acc:-1.1062475843829351\n",
      "step3: cost:2.0185345985371836, acc:-3.0120568344421654\n",
      "step4: cost:1.971924433445647, acc:-1.0574789744716977\n",
      "step5: cost:1.4071786183934334, acc:-1.449858018269465\n",
      "step6: cost:1.48186630660223, acc:-0.7014353080141951\n",
      "step7: cost:2.4975380260278657, acc:-1.0971078633055003\n",
      "step8: cost:1.1502932315201506, acc:-1.7534220581226556\n",
      "step9: cost:1.8725153600121285, acc:-1.913927955868202\n",
      "step10: cost:1.7086962538194128, acc:-1.7114920685878245\n",
      "step11: cost:1.7653021132018591, acc:-0.8532443445656301\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'babo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-9983bd4ccb3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mm1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLPNoTFModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"notf-iris-regression-hid-1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mm1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbabo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'babo' is not defined"
     ]
    }
   ],
   "source": [
    "m1 = MLPNoTFModel(\"notf-iris-regression-hid-1\", id1, [10])\n",
    "m1.train(epoch_count=1)\n",
    "print(babo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = MLPNoTFModel(\"notf-iris-regression-hid-1\", id1, [10])\n",
    "m1.train(epoch_count=100)\n",
    "\n",
    "m2 = MLPNoTFModel(\"notf-iris-binary-hid-3\", id2, [8,4,2])\n",
    "m2.train(epoch_count=100)\n",
    "\n",
    "m3 = MLPNoTFModel(\"notf-iris-select-hid-none\", id3, [])\n",
    "m3.train(epoch_count=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1.test()\n",
    "m2.test()\n",
    "m3.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1.demonstrate(5)\n",
    "m2.demonstrate(10)\n",
    "m3.demonstrate(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m4 = MLPNoTFModel(\"notf-mnist-hidden-1\", md, [32], 0.01)\n",
    "m4.train(epoch_count=10)\n",
    "\n",
    "m5 = MLPNoTFModel(\"notf-mnist-hidden-3\", md, [16,8,4], 0.01)\n",
    "m5.train(epoch_count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m4.test()\n",
    "m5.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m4.demonstrate(16, True)\n",
    "m5.demonstrate(16, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
