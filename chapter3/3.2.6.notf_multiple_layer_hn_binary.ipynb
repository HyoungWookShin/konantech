{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.8\n",
    "rand_std = 0.030\n",
    "learning_rate = 0.001\n",
    "epoch_count = 100\n",
    "report_period = 10\n",
    "random_fix = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dataset = datasets.load_iris()\n",
    "\n",
    "data = iris_dataset.data\n",
    "target = iris_dataset.target\n",
    "target_names = iris_dataset.target_names\n",
    "\n",
    "#print(\"dimension: data{}, target{}, target_names{}\".\n",
    "#    format(data.shape, target.shape, target_names.shape))\n",
    "#print(target_names)\n",
    "#print(data[:5])\n",
    "#print(target[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if random_fix: np.random.seed(1234)\n",
    "\n",
    "data_count = len(data)\n",
    "train_count = int(data_count * train_ratio)\n",
    "test_count = data_count - train_count\n",
    "    \n",
    "indices = np.arange(data_count)\n",
    "np.random.shuffle(indices)\n",
    "    \n",
    "train_data = data[indices[0:train_count]]\n",
    "train_target = target[indices[0:train_count]]\n",
    "    \n",
    "test_data = data[indices[train_count:data_count]]\n",
    "test_target = target[indices[train_count:data_count]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim, output_dim = 4, 1\n",
    "\n",
    "def get_test_data():\n",
    "    test_X = test_data\n",
    "    test_Y = np.zeros([test_count, 1])\n",
    "    \n",
    "    for i in range(test_count):\n",
    "        if test_target[i] == 0: test_Y[i, 0] = 1.0\n",
    "            \n",
    "    return test_X, test_Y\n",
    "\n",
    "def get_train_data(batch_size, nth):\n",
    "    global indices\n",
    "    \n",
    "    if nth == 0:\n",
    "        indices = np.arange(train_count)\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "    from_idx = nth * batch_size\n",
    "    to_idx = (nth + 1) * batch_size\n",
    "    \n",
    "    train_X = train_data[indices[from_idx:to_idx]]\n",
    "    train_Y = np.zeros([batch_size, 1])\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        k = indices[from_idx+i]\n",
    "        if train_target[k] == 0: train_Y[i, 0] = 1.0\n",
    "            \n",
    "    return train_X, train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_rand_normal(in_dim, out_dim):\n",
    "    init_64 = np.random.normal(0, rand_std, [in_dim, out_dim])\n",
    "    init = init_64.astype('float32')\n",
    "\n",
    "    return init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dims = [8, 4, 2]\n",
    "\n",
    "def init_parameter():\n",
    "    if random_fix: np.random.seed(9876)\n",
    "\n",
    "    global weights, biases\n",
    "    \n",
    "    weights, biases = [], []\n",
    "    prev_dim = input_dim\n",
    "\n",
    "    for n in range(len(hidden_dims)):\n",
    "        next_dim = hidden_dims[n]\n",
    "        w = init_rand_normal(prev_dim, next_dim)\n",
    "        b = np.zeros([next_dim])\n",
    "        weights.append(w)\n",
    "        biases.append(b)\n",
    "        prev_dim = next_dim\n",
    "\n",
    "    w = init_rand_normal(prev_dim, output_dim)\n",
    "    b = np.zeros([output_dim])\n",
    "    weights.append(w)\n",
    "    biases.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x, 0)     # np.max(x, 0)\n",
    "\n",
    "def relu_derv(y):\n",
    "    return np.sign(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_forward(x):\n",
    "    global weights, biases\n",
    "    global hiddens\n",
    "    \n",
    "    hiddens = [x]\n",
    "\n",
    "    for n in range(len(hidden_dims)):\n",
    "        hid = relu(np.matmul(hiddens[-1], weights[n]) + biases[n])\n",
    "        hiddens.append(hid)\n",
    "    \n",
    "    output = np.matmul(hiddens[-1], weights[-1]) + biases[-1]\n",
    "    \n",
    "    return output\n",
    "\n",
    "def proc_backward(x, grad):\n",
    "    global weights, biases\n",
    "    global hiddens\n",
    "    \n",
    "    w_out_derv = hiddens[-1].transpose()\n",
    "    w_out_grad = np.matmul(w_out_derv, grad)\n",
    "    \n",
    "    b_out_grad = np.sum(grad, axis=0)\n",
    "    \n",
    "    hidden_derv = weights[-1].transpose()\n",
    "    hidden_grad = np.matmul(grad, hidden_derv)\n",
    "    \n",
    "    for n in range(len(hidden_dims))[::-1]:\n",
    "        hidden_affine_derv = relu_derv(hiddens[n+1])\n",
    "        hidden_affine_grad = hidden_affine_derv * hidden_grad\n",
    "    \n",
    "        w_hid_derv = hiddens[n].transpose()\n",
    "        w_hid_grad = np.matmul(w_hid_derv, hidden_affine_grad)\n",
    "    \n",
    "        b_hid_grad = np.sum(hidden_affine_grad, axis=0)\n",
    "        \n",
    "        grad = hidden_affine_grad\n",
    "    \n",
    "        hidden_derv = weights[n].transpose()\n",
    "        hidden_grad = np.matmul(grad, hidden_derv)\n",
    "    \n",
    "        weights[n] = weights[n] - learning_rate * w_hid_grad\n",
    "        biases[n] = biases[n] - learning_rate * b_hid_grad\n",
    "    \n",
    "    weights[-1] = weights[-1] - learning_rate * w_out_grad\n",
    "    biases[-1] = biases[-1] - learning_rate * b_out_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derv(x, y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "def sigmoid_cross_entropy(z, x):\n",
    "    return np.maximum(x, 0) - x * z + np.log(1 + np.exp(-np.abs(x)))\n",
    "\n",
    "def sigmoid_cross_entropy_derv(z, x):\n",
    "    return -z + sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_accuracy(output, y):\n",
    "    #probs = sigmoid(output)\n",
    "    #estimate = np.greater(probs, 0.5)\n",
    "    estimate = np.greater(output, 0)\n",
    "    answer = np.equal(y, 1.0)\n",
    "    correct = np.equal(estimate, answer)\n",
    "    return np.mean(correct)\n",
    "\n",
    "def test(x, y):\n",
    "    output = proc_forward(x)\n",
    "    return eval_accuracy(output, y)\n",
    "\n",
    "def train_step(x, y):\n",
    "    output = proc_forward(x)\n",
    "    \n",
    "    entropy = sigmoid_cross_entropy(y, output)\n",
    "    loss = np.mean(entropy)\n",
    "    \n",
    "    loss_grad = 1.0\n",
    "    \n",
    "    ent_derv = np.ones_like(entropy) / np.prod(entropy.shape)\n",
    "    ent_grad = ent_derv * loss_grad\n",
    "    \n",
    "    output_derv = sigmoid_cross_entropy_derv(y, output)\n",
    "    output_grad = output_derv * ent_grad\n",
    "    \n",
    "    proc_backward(x, output_grad)\n",
    "    \n",
    "    return loss, eval_accuracy(output, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(batch_size=0):\n",
    "    if batch_size == 0: batch_size = train_count\n",
    "    batch_count = int(train_count / batch_size)\n",
    "    test_X, test_Y = get_test_data()\n",
    "        \n",
    "    init_parameter()\n",
    "    \n",
    "    if random_fix: np.random.seed(1945)\n",
    "        \n",
    "    for epoch in range(epoch_count):\n",
    "        costs = []\n",
    "        accs = []\n",
    "        for n in range(batch_count):\n",
    "            train_X, train_Y = get_train_data(batch_size, n)\n",
    "            cost, acc = train_step(train_X, train_Y)\n",
    "            costs.append(cost)\n",
    "            accs.append(acc)\n",
    "            \n",
    "        if (epoch+1) % report_period == 0:\n",
    "            acc = test(test_X, test_Y)\n",
    "            print(\"Epoch {}: cost={:15.13f}, accuracy={:5.3f}/{:5.3f}\". \\\n",
    "                  format(epoch+1, np.mean(costs), np.mean(accs), acc))\n",
    "            \n",
    "    final_acc = test(test_X, test_Y)\n",
    "    print(\"\\nFinal Test: final accuracy = {:5.3f}\".format(final_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: cost=0.6897798219634, accuracy=0.675/0.633\n",
      "Epoch 20: cost=0.6864083343055, accuracy=0.675/0.633\n",
      "Epoch 30: cost=0.6832332285909, accuracy=0.675/0.633\n",
      "Epoch 40: cost=0.6802443644089, accuracy=0.675/0.633\n",
      "Epoch 50: cost=0.6774262947355, accuracy=0.675/0.633\n",
      "Epoch 60: cost=0.6747725369672, accuracy=0.675/0.633\n",
      "Epoch 70: cost=0.6722759719072, accuracy=0.675/0.633\n",
      "Epoch 80: cost=0.6699219704177, accuracy=0.675/0.633\n",
      "Epoch 90: cost=0.6677071555779, accuracy=0.675/0.633\n",
      "Epoch 100: cost=0.6656195421616, accuracy=0.675/0.633\n",
      "\n",
      "Final Test: final accuracy = 0.633\n"
     ]
    }
   ],
   "source": [
    "train_and_test(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: cost=0.6928712777588, accuracy=0.675/0.633\n",
      "Epoch 20: cost=0.6925671249712, accuracy=0.675/0.633\n",
      "Epoch 30: cost=0.6922644893329, accuracy=0.675/0.633\n",
      "Epoch 40: cost=0.6919633632710, accuracy=0.675/0.633\n",
      "Epoch 50: cost=0.6916637392479, accuracy=0.675/0.633\n",
      "Epoch 60: cost=0.6913656097614, accuracy=0.675/0.633\n",
      "Epoch 70: cost=0.6910689673447, accuracy=0.675/0.633\n",
      "Epoch 80: cost=0.6907738045664, accuracy=0.675/0.633\n",
      "Epoch 90: cost=0.6904801140299, accuracy=0.675/0.633\n",
      "Epoch 100: cost=0.6901878883736, accuracy=0.675/0.633\n",
      "\n",
      "Final Test: final accuracy = 0.633\n"
     ]
    }
   ],
   "source": [
    "train_and_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: cost=0.6666757873531, accuracy=0.675/0.633\n",
      "Epoch 20: cost=0.6509473533335, accuracy=0.675/0.633\n",
      "Epoch 30: cost=0.6422044010868, accuracy=0.675/0.633\n",
      "Epoch 40: cost=0.6372829456182, accuracy=0.675/0.633\n",
      "Epoch 50: cost=0.6344951043511, accuracy=0.675/0.633\n",
      "Epoch 60: cost=0.6328960461727, accuracy=0.675/0.633\n",
      "Epoch 70: cost=0.6319734409037, accuracy=0.675/0.633\n",
      "Epoch 80: cost=0.6314401544122, accuracy=0.675/0.633\n",
      "Epoch 90: cost=0.6311330676229, accuracy=0.675/0.633\n",
      "Epoch 100: cost=0.6309508666343, accuracy=0.675/0.633\n",
      "\n",
      "Final Test: final accuracy = 0.633\n"
     ]
    }
   ],
   "source": [
    "train_and_test(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
