{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.8\n",
    "rand_std = 0.030\n",
    "learning_rate = 0.001\n",
    "epoch_count = 100\n",
    "report_period = 10\n",
    "random_fix = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dataset = datasets.load_iris()\n",
    "\n",
    "data = iris_dataset.data\n",
    "target = iris_dataset.target\n",
    "target_names = iris_dataset.target_names\n",
    "\n",
    "#print(\"dimension: data{}, target{}, target_names{}\".\n",
    "#    format(data.shape, target.shape, target_names.shape))\n",
    "#print(target_names)\n",
    "#print(data[:5])\n",
    "#print(target[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if random_fix: np.random.seed(1234)\n",
    "\n",
    "data_count = len(data)\n",
    "train_count = int(data_count * train_ratio)\n",
    "test_count = data_count - train_count\n",
    "    \n",
    "indices = np.arange(data_count)\n",
    "np.random.shuffle(indices)\n",
    "    \n",
    "train_data = data[indices[0:train_count]]\n",
    "train_target = target[indices[0:train_count]]\n",
    "    \n",
    "test_data = data[indices[train_count:data_count]]\n",
    "test_target = target[indices[train_count:data_count]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim, output_dim = 3, 1\n",
    "\n",
    "def get_test_data():\n",
    "    test_X = test_data[:, 0:3]\n",
    "    test_Y = test_data[:, 3:4]\n",
    "\n",
    "    return test_X, test_Y\n",
    "\n",
    "def get_train_data(batch_size, nth):\n",
    "    global indices\n",
    "    \n",
    "    if nth == 0:\n",
    "        indices = np.arange(train_count)\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "    from_idx = nth * batch_size\n",
    "    to_idx = (nth + 1) * batch_size\n",
    "    \n",
    "    train_X = train_data[indices[from_idx:to_idx], 0:3]\n",
    "    train_Y = train_data[indices[from_idx:to_idx], 3:4]   \n",
    "    \n",
    "    return train_X, train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_rand_normal(in_dim, out_dim):\n",
    "    init_64 = np.random.normal(0, rand_std, [in_dim, out_dim])\n",
    "    init = init_64.astype('float32')\n",
    "\n",
    "    return init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dims = [8, 4, 2]\n",
    "\n",
    "def init_parameter():\n",
    "    if random_fix: np.random.seed(9876)\n",
    "\n",
    "    global weights, biases\n",
    "    \n",
    "    weights, biases = [], []\n",
    "    prev_dim = input_dim\n",
    "\n",
    "    for n in range(len(hidden_dims)):\n",
    "        next_dim = hidden_dims[n]\n",
    "        w = init_rand_normal(prev_dim, next_dim)\n",
    "        b = np.zeros([next_dim])\n",
    "        weights.append(w)\n",
    "        biases.append(b)\n",
    "        prev_dim = next_dim\n",
    "\n",
    "    w = init_rand_normal(prev_dim, output_dim)\n",
    "    b = np.zeros([output_dim])\n",
    "    weights.append(w)\n",
    "    biases.append(b)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x, 0)     # np.max(x, 0)\n",
    "\n",
    "def relu_derv(y):\n",
    "    return np.sign(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_forward(x):\n",
    "    global weights, biases\n",
    "    global hiddens\n",
    "    \n",
    "    hiddens = [x]\n",
    "\n",
    "    for n in range(len(hidden_dims)):\n",
    "        hid = relu(np.matmul(hiddens[-1], weights[n]) + biases[n])\n",
    "        hiddens.append(hid)\n",
    "    \n",
    "    output = np.matmul(hiddens[-1], weights[-1]) + biases[-1]\n",
    "    \n",
    "    return output\n",
    "\n",
    "def proc_backward(x, grad):\n",
    "    global weights, biases\n",
    "    global hiddens\n",
    "    \n",
    "    w_out_derv = hiddens[-1].transpose()\n",
    "    w_out_grad = np.matmul(w_out_derv, grad)\n",
    "    \n",
    "    b_out_grad = np.sum(grad, axis=0)\n",
    "    \n",
    "    hidden_derv = weights[-1].transpose()\n",
    "    hidden_grad = np.matmul(grad, hidden_derv)\n",
    "    \n",
    "    for n in range(len(hidden_dims))[::-1]:\n",
    "        hidden_affine_derv = relu_derv(hiddens[n+1])\n",
    "        hidden_affine_grad = hidden_affine_derv * hidden_grad\n",
    "    \n",
    "        w_hid_derv = hiddens[n].transpose()\n",
    "        w_hid_grad = np.matmul(w_hid_derv, hidden_affine_grad)\n",
    "    \n",
    "        b_hid_grad = np.sum(hidden_affine_grad, axis=0)\n",
    "        \n",
    "        grad = hidden_affine_grad\n",
    "    \n",
    "        hidden_derv = weights[n].transpose()\n",
    "        hidden_grad = np.matmul(grad, hidden_derv)\n",
    "    \n",
    "        weights[n] = weights[n] - learning_rate * w_hid_grad\n",
    "        biases[n] = biases[n] - learning_rate * b_hid_grad\n",
    "    \n",
    "    weights[-1] = weights[-1] - learning_rate * w_out_grad\n",
    "    biases[-1] = biases[-1] - learning_rate * b_out_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_accuracy(output, y):\n",
    "    return 1 - np.mean(np.abs(output-y) / y)\n",
    "\n",
    "def test(x, y):\n",
    "    output = proc_forward(x)\n",
    "    return eval_accuracy(output, y)\n",
    "\n",
    "def train_step(x, y):\n",
    "    output = proc_forward(x)\n",
    "    diff = output - y\n",
    "    power = np.power(diff, 2)\n",
    "    loss = np.mean(power)\n",
    "    \n",
    "    loss_grad = 1.0\n",
    "    \n",
    "    power_derv = np.ones_like(y) / np.prod(y.shape)\n",
    "    power_grad = power_derv * loss_grad\n",
    "    \n",
    "    diff_derv = 2 * diff\n",
    "    diff_grad = diff_derv * power_grad\n",
    "    \n",
    "    output_derv = 1\n",
    "    output_grad = output_derv * diff_grad\n",
    "    \n",
    "    proc_backward(x, output_grad)\n",
    "    \n",
    "    return loss, eval_accuracy(output, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(batch_size=0):\n",
    "    if batch_size == 0: batch_size = train_count\n",
    "    batch_count = int(train_count / batch_size)\n",
    "    test_X, test_Y = get_test_data()\n",
    "        \n",
    "    init_parameter()\n",
    "    \n",
    "    if random_fix: np.random.seed(1945)\n",
    "        \n",
    "    for epoch in range(epoch_count):\n",
    "        costs = []\n",
    "        accs = []\n",
    "        for n in range(batch_count):\n",
    "            train_X, train_Y = get_train_data(batch_size, n)\n",
    "            cost, acc = train_step(train_X, train_Y)\n",
    "            costs.append(cost)\n",
    "            accs.append(acc)\n",
    "            \n",
    "        if (epoch+1) % report_period == 0:\n",
    "            acc = test(test_X, test_Y)\n",
    "            print(\"Epoch {}: cost={:5.3f}, accuracy={:5.3f}/{:5.3f}\". \\\n",
    "                  format(epoch+1, np.mean(costs), np.mean(accs), acc))\n",
    "            \n",
    "    final_acc = test(test_X, test_Y)\n",
    "    print(\"\\nFinal Test: final accuracy = {:5.3f}\".format(final_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: cost=1.456, accuracy=0.318/0.229\n",
      "Epoch 20: cost=1.111, accuracy=0.161/-0.055\n",
      "Epoch 30: cost=0.897, accuracy=-0.003/-0.320\n",
      "Epoch 40: cost=0.765, accuracy=-0.139/-0.533\n",
      "Epoch 50: cost=0.683, accuracy=-0.243/-0.700\n",
      "Epoch 60: cost=0.633, accuracy=-0.327/-0.832\n",
      "Epoch 70: cost=0.601, accuracy=-0.394/-0.936\n",
      "Epoch 80: cost=0.582, accuracy=-0.447/-1.017\n",
      "Epoch 90: cost=0.570, accuracy=-0.493/-1.081\n",
      "Epoch 100: cost=0.563, accuracy=-0.528/-1.131\n",
      "\n",
      "Final Test: final accuracy = -1.131\n"
     ]
    }
   ],
   "source": [
    "train_and_test(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: cost=1.926, accuracy=0.042/0.059\n",
      "Epoch 20: cost=1.872, accuracy=0.087/0.116\n",
      "Epoch 30: cost=1.820, accuracy=0.131/0.172\n",
      "Epoch 40: cost=1.770, accuracy=0.175/0.227\n",
      "Epoch 50: cost=1.723, accuracy=0.212/0.254\n",
      "Epoch 60: cost=1.677, accuracy=0.243/0.264\n",
      "Epoch 70: cost=1.632, accuracy=0.274/0.274\n",
      "Epoch 80: cost=1.590, accuracy=0.303/0.283\n",
      "Epoch 90: cost=1.549, accuracy=0.333/0.293\n",
      "Epoch 100: cost=1.510, accuracy=0.333/0.274\n",
      "\n",
      "Final Test: final accuracy = 0.274\n"
     ]
    }
   ],
   "source": [
    "train_and_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: cost=0.567, accuracy=-0.520/-1.127\n",
      "Epoch 20: cost=0.552, accuracy=-0.662/-1.307\n",
      "Epoch 30: cost=0.552, accuracy=-0.660/-1.326\n",
      "Epoch 40: cost=0.552, accuracy=-0.668/-1.327\n",
      "Epoch 50: cost=0.552, accuracy=-0.653/-1.330\n",
      "Epoch 60: cost=0.552, accuracy=-0.653/-1.331\n",
      "Epoch 70: cost=0.552, accuracy=-0.662/-1.327\n",
      "Epoch 80: cost=0.552, accuracy=-0.651/-1.329\n",
      "Epoch 90: cost=0.552, accuracy=-0.663/-1.322\n",
      "Epoch 100: cost=0.552, accuracy=-0.657/-1.327\n",
      "\n",
      "Final Test: final accuracy = -1.327\n"
     ]
    }
   ],
   "source": [
    "train_and_test(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
